{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74da2e09-dd45-41f9-bb04-55a1a02d702b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, ConfusionMatrixDisplay\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def perform_neural_network_classification(csv_file_path):\n",
    "    \"\"\"\n",
    "    Performs neural network binary classification on a dataset, calculates\n",
    "    various classification metrics, generates a confusion matrix heatmap,\n",
    "    and saves the metrics to an Excel file.\n",
    "\n",
    "    Args:\n",
    "        csv_file_path (str): The path to the CSV file. All columns except the last\n",
    "                             are treated as features (X), and the last column,\n",
    "                             which should contain 0s and 1s, is the target variable (y).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the dataset\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "        # Separate features (X) and target (y)\n",
    "        X = df.iloc[:, :-1]  # All columns except the last\n",
    "        y = df.iloc[:, -1]   # The last column (0 or 1)\n",
    "\n",
    "        # Split the data into training and testing sets (80/20 split)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Standardize the data to help the neural network converge faster\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # --- Neural Network Model Setup ---\n",
    "        # Define a sequential neural network model\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Input layer and first hidden layer with 64 neurons and ReLU activation\n",
    "        model.add(Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "        \n",
    "        # Dropout layer to prevent overfitting\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "        # Second hidden layer with 32 neurons and ReLU activation\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        \n",
    "        # Output layer for binary classification with a single neuron and sigmoid activation\n",
    "        # Sigmoid function squashes the output to a probability between 0 and 1\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        # Compile the model with the Adam optimizer and binary cross-entropy loss\n",
    "        # Binary cross-entropy is the standard loss function for binary classification\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Print the model summary\n",
    "        print(\"Model Summary:\")\n",
    "        model.summary()\n",
    "        \n",
    "        # Train the model\n",
    "        print(\"\\nTraining Neural Network model...\")\n",
    "        history = model.fit(\n",
    "            X_train_scaled, y_train,\n",
    "            epochs=50,  # Number of training epochs\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,  # Use 20% of the training data for validation\n",
    "            verbose=1  # Show training progress\n",
    "        )\n",
    "        \n",
    "        # --- Make Predictions ---\n",
    "        # The model predicts a probability. We round it to get a binary class (0 or 1).\n",
    "        y_pred_proba = model.predict(X_test_scaled)\n",
    "        y_pred = (y_pred_proba > 0.5).astype(\"int32\").flatten()\n",
    "\n",
    "        # --- 1. Calculate Classification Metrics ---\n",
    "        print(\"\\n--- Neural Network Model Performance Metrics ---\")\n",
    "\n",
    "        # Accuracy Score\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # Classification Report (Precision, Recall, F1-Score)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        # ROC AUC Score\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "        # --- 2. Save Metrics to Excel ---\n",
    "        # Create a dictionary to hold the metrics\n",
    "        metrics_data = {\n",
    "            'Metric': ['Accuracy', 'ROC AUC Score'],\n",
    "            'Value': [accuracy, roc_auc]\n",
    "        }\n",
    "        \n",
    "        metrics_df = pd.DataFrame(metrics_data)\n",
    "        excel_path = 'nn_classification_performance.xlsx'\n",
    "        metrics_df.to_excel(excel_path, index=False)\n",
    "        print(f\"\\nModel performance metrics saved to '{excel_path}'\")\n",
    "\n",
    "        # --- 3. Generate Confusion Matrix Plot ---\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        plt.figure(figsize=(8, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                    xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "                    yticklabels=['Actual 0', 'Actual 1'])\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        \n",
    "        plot_path = 'confusion_matrix.svg'\n",
    "        plt.savefig(plot_path, format='svg')\n",
    "        print(f\"\\nConfusion matrix plot saved to '{plot_path}'\")\n",
    "        plt.show()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{csv_file_path}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9f4ca6b-2142-4893-a88f-6444bc5195e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Model Summary:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                896       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3009 (11.75 KB)\n",
      "Trainable params: 3009 (11.75 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "Training Neural Network model...\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "200/200 [==============================] - 4s 6ms/step - loss: 0.6999 - accuracy: 0.5003 - val_loss: 0.6961 - val_accuracy: 0.4969\n",
      "Epoch 2/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6924 - accuracy: 0.5181 - val_loss: 0.6962 - val_accuracy: 0.4881\n",
      "Epoch 3/50\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6912 - accuracy: 0.5230 - val_loss: 0.6972 - val_accuracy: 0.4944\n",
      "Epoch 4/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6889 - accuracy: 0.5394 - val_loss: 0.6975 - val_accuracy: 0.4825\n",
      "Epoch 5/50\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6902 - accuracy: 0.5334 - val_loss: 0.6956 - val_accuracy: 0.4981\n",
      "Epoch 6/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6883 - accuracy: 0.5405 - val_loss: 0.6947 - val_accuracy: 0.5113\n",
      "Epoch 7/50\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6898 - accuracy: 0.5452 - val_loss: 0.6956 - val_accuracy: 0.5025\n",
      "Epoch 8/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6872 - accuracy: 0.5441 - val_loss: 0.6958 - val_accuracy: 0.4931\n",
      "Epoch 9/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6857 - accuracy: 0.5522 - val_loss: 0.6954 - val_accuracy: 0.4981\n",
      "Epoch 10/50\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6851 - accuracy: 0.5508 - val_loss: 0.6957 - val_accuracy: 0.5025\n",
      "Epoch 11/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6831 - accuracy: 0.5583 - val_loss: 0.6972 - val_accuracy: 0.5063\n",
      "Epoch 12/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6825 - accuracy: 0.5547 - val_loss: 0.6964 - val_accuracy: 0.5056\n",
      "Epoch 13/50\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6806 - accuracy: 0.5609 - val_loss: 0.6973 - val_accuracy: 0.4956\n",
      "Epoch 14/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6805 - accuracy: 0.5598 - val_loss: 0.6997 - val_accuracy: 0.5138\n",
      "Epoch 15/50\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6801 - accuracy: 0.5641 - val_loss: 0.6980 - val_accuracy: 0.5106\n",
      "Epoch 16/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6796 - accuracy: 0.5595 - val_loss: 0.6992 - val_accuracy: 0.5131\n",
      "Epoch 17/50\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6804 - accuracy: 0.5577 - val_loss: 0.6982 - val_accuracy: 0.5113\n",
      "Epoch 18/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6777 - accuracy: 0.5694 - val_loss: 0.6988 - val_accuracy: 0.5138\n",
      "Epoch 19/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6787 - accuracy: 0.5645 - val_loss: 0.6971 - val_accuracy: 0.5181\n",
      "Epoch 20/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6777 - accuracy: 0.5663 - val_loss: 0.6975 - val_accuracy: 0.5231\n",
      "Epoch 21/50\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6731 - accuracy: 0.5800 - val_loss: 0.6989 - val_accuracy: 0.5213\n",
      "Epoch 22/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6751 - accuracy: 0.5734 - val_loss: 0.6987 - val_accuracy: 0.5150\n",
      "Epoch 23/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6731 - accuracy: 0.5784 - val_loss: 0.6994 - val_accuracy: 0.5113\n",
      "Epoch 24/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6715 - accuracy: 0.5839 - val_loss: 0.7010 - val_accuracy: 0.5063\n",
      "Epoch 25/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6716 - accuracy: 0.5758 - val_loss: 0.7027 - val_accuracy: 0.5075\n",
      "Epoch 26/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6714 - accuracy: 0.5791 - val_loss: 0.7011 - val_accuracy: 0.5094\n",
      "Epoch 27/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6696 - accuracy: 0.5806 - val_loss: 0.7028 - val_accuracy: 0.5094\n",
      "Epoch 28/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6725 - accuracy: 0.5892 - val_loss: 0.7034 - val_accuracy: 0.5088\n",
      "Epoch 29/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6686 - accuracy: 0.5911 - val_loss: 0.7036 - val_accuracy: 0.5100\n",
      "Epoch 30/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6697 - accuracy: 0.5822 - val_loss: 0.7038 - val_accuracy: 0.5144\n",
      "Epoch 31/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6665 - accuracy: 0.5959 - val_loss: 0.7043 - val_accuracy: 0.5081\n",
      "Epoch 32/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6667 - accuracy: 0.5913 - val_loss: 0.7037 - val_accuracy: 0.5075\n",
      "Epoch 33/50\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6661 - accuracy: 0.5902 - val_loss: 0.7059 - val_accuracy: 0.5075\n",
      "Epoch 34/50\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6657 - accuracy: 0.5920 - val_loss: 0.7081 - val_accuracy: 0.5025\n",
      "Epoch 35/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6656 - accuracy: 0.5875 - val_loss: 0.7072 - val_accuracy: 0.5050\n",
      "Epoch 36/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6638 - accuracy: 0.5964 - val_loss: 0.7069 - val_accuracy: 0.5044\n",
      "Epoch 37/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6632 - accuracy: 0.5902 - val_loss: 0.7055 - val_accuracy: 0.5025\n",
      "Epoch 38/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6622 - accuracy: 0.5991 - val_loss: 0.7074 - val_accuracy: 0.5006\n",
      "Epoch 39/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.5981 - val_loss: 0.7090 - val_accuracy: 0.5038\n",
      "Epoch 40/50\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6616 - accuracy: 0.5983 - val_loss: 0.7113 - val_accuracy: 0.5088\n",
      "Epoch 41/50\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6635 - accuracy: 0.5975 - val_loss: 0.7094 - val_accuracy: 0.4975\n",
      "Epoch 42/50\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6604 - accuracy: 0.6016 - val_loss: 0.7095 - val_accuracy: 0.5106\n",
      "Epoch 43/50\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6559 - accuracy: 0.6081 - val_loss: 0.7123 - val_accuracy: 0.5038\n",
      "Epoch 44/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6012 - val_loss: 0.7151 - val_accuracy: 0.4988\n",
      "Epoch 45/50\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6614 - accuracy: 0.5964 - val_loss: 0.7126 - val_accuracy: 0.4988\n",
      "Epoch 46/50\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6569 - accuracy: 0.5973 - val_loss: 0.7124 - val_accuracy: 0.5025\n",
      "Epoch 47/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6153 - val_loss: 0.7136 - val_accuracy: 0.5063\n",
      "Epoch 48/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6064 - val_loss: 0.7113 - val_accuracy: 0.5031\n",
      "Epoch 49/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6091 - val_loss: 0.7119 - val_accuracy: 0.5063\n",
      "Epoch 50/50\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6596 - accuracy: 0.6003 - val_loss: 0.7104 - val_accuracy: 0.5219\n",
      "63/63 [==============================] - 0s 2ms/step\n",
      "\n",
      "--- Neural Network Model Performance Metrics ---\n",
      "Accuracy: 0.4955\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.36      0.41       989\n",
      "         1.0       0.50      0.63      0.56      1011\n",
      "\n",
      "    accuracy                           0.50      2000\n",
      "   macro avg       0.49      0.49      0.48      2000\n",
      "weighted avg       0.49      0.50      0.49      2000\n",
      "\n",
      "ROC AUC Score: 0.4979\n",
      "\n",
      "Model performance metrics saved to 'nn_classification_performance.xlsx'\n",
      "\n",
      "Confusion matrix plot saved to 'confusion_matrix.svg'\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perform_neural_network_classification(\"../mapped_dataset_Normalized_version.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db3b3f7a-b8fb-436d-b5a4-324deb0cfa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, classification_report,\n",
    "    roc_auc_score, r2_score, mean_squared_error, mean_absolute_error,\n",
    "    mean_absolute_percentage_error, mean_squared_log_error\n",
    ")\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def perform_neural_network_classification(csv_file_path):\n",
    "    \"\"\"\n",
    "    Performs neural network binary classification on a dataset, calculates\n",
    "    various classification and regression metrics, generates a confusion\n",
    "    matrix heatmap, and saves all metrics to an Excel file.\n",
    "\n",
    "    Args:\n",
    "        csv_file_path (str): The path to the CSV file. All columns except the last\n",
    "                             are treated as features (X), and the last column,\n",
    "                             which should contain 0s and 1s, is the target variable (y).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the dataset\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "        # Separate features (X) and target (y)\n",
    "        X = df.iloc[:, :-1]  # All columns except the last\n",
    "        y = df.iloc[:, -1]   # The last column (0 or 1)\n",
    "\n",
    "        # Split the data into training and testing sets (80/20 split)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Standardize the data to help the neural network converge faster\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # --- Neural Network Model Setup ---\n",
    "        # Define a more complex sequential neural network model\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Input layer and first hidden layer with 128 neurons and ReLU activation\n",
    "        model.add(Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "        \n",
    "        # Dropout layer to prevent overfitting\n",
    "        model.add(Dropout(0.4))\n",
    "        \n",
    "        # Second hidden layer with 64 neurons and ReLU activation\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        \n",
    "        # Output layer for binary classification with a single neuron and sigmoid activation\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        # Compile the model with the Adam optimizer and binary cross-entropy loss\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Print the model summary\n",
    "        print(\"Model Summary:\")\n",
    "        model.summary()\n",
    "        \n",
    "        # Train the model with more epochs\n",
    "        print(\"\\nTraining Neural Network model...\")\n",
    "        history = model.fit(\n",
    "            X_train_scaled, y_train,\n",
    "            epochs=100,  # Increased number of training epochs\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,  # Use 20% of the training data for validation\n",
    "            verbose=1  # Show training progress\n",
    "        )\n",
    "        \n",
    "        # --- Make Predictions ---\n",
    "        # The model predicts a probability. We round it to get a binary class (0 or 1).\n",
    "        y_pred_proba = model.predict(X_test_scaled).flatten()\n",
    "        y_pred = (y_pred_proba > 0.5).astype(\"int32\")\n",
    "\n",
    "        # --- 1. Calculate Classification Metrics ---\n",
    "        print(\"\\n--- Neural Network Model Performance Metrics ---\")\n",
    "\n",
    "        # Accuracy Score\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # Classification Report (Precision, Recall, F1-Score)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        # ROC AUC Score\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "        \n",
    "        # --- 2. Calculate Regression Metrics on Probabilities ---\n",
    "        print(\"\\n--- Regression Metrics on Predicted Probabilities ---\")\n",
    "\n",
    "        # R-squared (Coefficient of Determination)\n",
    "        r2 = r2_score(y_test, y_pred_proba)\n",
    "        print(f\"R-squared (R2): {r2:.4f}\")\n",
    "\n",
    "        # Mean Absolute Error (MAE)\n",
    "        mae = mean_absolute_error(y_test, y_pred_proba)\n",
    "        print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "\n",
    "        # Mean Squared Error (MSE)\n",
    "        mse = mean_squared_error(y_test, y_pred_proba)\n",
    "        print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "\n",
    "        # Root Mean Squared Error (RMSE)\n",
    "        rmse = np.sqrt(mse)\n",
    "        print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "\n",
    "        # Mean Absolute Percentage Error (MAPE)\n",
    "        # Add a small epsilon to avoid division by zero\n",
    "        mape = np.mean(np.abs((y_test - y_pred_proba) / (y_test + 1e-8))) * 100\n",
    "        print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "\n",
    "        # Mean Squared Log Error (MSLE) - check for negative values\n",
    "        # Add a small value to predictions to avoid log(0)\n",
    "        msle = mean_squared_log_error(y_test + 1e-8, y_pred_proba + 1e-8)\n",
    "        print(f\"Mean Squared Log Error (MSLE): {msle:.4f}\")\n",
    "\n",
    "        # --- 3. Save Metrics to Excel ---\n",
    "        # Create a dictionary to hold the metrics\n",
    "        metrics_data = {\n",
    "            'Metric': ['Accuracy', 'ROC AUC', 'R2 Score', 'MAE', 'MSE', 'RMSE', 'MAPE', 'MSLE'],\n",
    "            'Value': [accuracy, roc_auc, r2, mae, mse, rmse, mape, msle]\n",
    "        }\n",
    "        \n",
    "        metrics_df = pd.DataFrame(metrics_data)\n",
    "        excel_path = 'nn_performance_metrics.xlsx'\n",
    "        metrics_df.to_excel(excel_path, index=False)\n",
    "        print(f\"\\nModel performance metrics saved to '{excel_path}'\")\n",
    "\n",
    "        # --- 4. Generate Confusion Matrix Plot ---\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        plt.figure(figsize=(8, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                    xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "                    yticklabels=['Actual 0', 'Actual 1'])\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        \n",
    "        plot_path = 'confusion_matrix.svg'\n",
    "        plt.savefig(plot_path, format='svg')\n",
    "        print(f\"\\nConfusion matrix plot saved to '{plot_path}'\")\n",
    "        plt.show()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{csv_file_path}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8dc1106-a06c-4892-812c-bdd6ed680562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary:\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 128)               1792      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10113 (39.50 KB)\n",
      "Trainable params: 10113 (39.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "Training Neural Network model...\n",
      "Epoch 1/100\n",
      "200/200 [==============================] - 3s 5ms/step - loss: 0.7022 - accuracy: 0.4981 - val_loss: 0.6967 - val_accuracy: 0.4969\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.6948 - accuracy: 0.5148 - val_loss: 0.6967 - val_accuracy: 0.5150\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6925 - accuracy: 0.5241 - val_loss: 0.6964 - val_accuracy: 0.5013\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6884 - accuracy: 0.5409 - val_loss: 0.6941 - val_accuracy: 0.5206\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6889 - accuracy: 0.5330 - val_loss: 0.6935 - val_accuracy: 0.5238\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6853 - accuracy: 0.5525 - val_loss: 0.6958 - val_accuracy: 0.5113\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6849 - accuracy: 0.5517 - val_loss: 0.6956 - val_accuracy: 0.5163\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6855 - accuracy: 0.5569 - val_loss: 0.6941 - val_accuracy: 0.5131\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6823 - accuracy: 0.5608 - val_loss: 0.6957 - val_accuracy: 0.5094\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6803 - accuracy: 0.5634 - val_loss: 0.6963 - val_accuracy: 0.5138\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6815 - accuracy: 0.5612 - val_loss: 0.6974 - val_accuracy: 0.5231\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6801 - accuracy: 0.5622 - val_loss: 0.6959 - val_accuracy: 0.5156\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6799 - accuracy: 0.5625 - val_loss: 0.6978 - val_accuracy: 0.5231\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6788 - accuracy: 0.5653 - val_loss: 0.6957 - val_accuracy: 0.5256\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6771 - accuracy: 0.5713 - val_loss: 0.6966 - val_accuracy: 0.5188\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6781 - accuracy: 0.5614 - val_loss: 0.6990 - val_accuracy: 0.5225\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6761 - accuracy: 0.5788 - val_loss: 0.6978 - val_accuracy: 0.5100\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6742 - accuracy: 0.5780 - val_loss: 0.6997 - val_accuracy: 0.5006\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6733 - accuracy: 0.5781 - val_loss: 0.7021 - val_accuracy: 0.5056\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6711 - accuracy: 0.5883 - val_loss: 0.7025 - val_accuracy: 0.4988\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6691 - accuracy: 0.5900 - val_loss: 0.7033 - val_accuracy: 0.5025\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6680 - accuracy: 0.5888 - val_loss: 0.7061 - val_accuracy: 0.4994\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6673 - accuracy: 0.5942 - val_loss: 0.7100 - val_accuracy: 0.4944\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6674 - accuracy: 0.5888 - val_loss: 0.7077 - val_accuracy: 0.5106\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.5962 - val_loss: 0.7054 - val_accuracy: 0.5194\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6623 - accuracy: 0.6014 - val_loss: 0.7068 - val_accuracy: 0.5150\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6626 - accuracy: 0.5989 - val_loss: 0.7136 - val_accuracy: 0.5150\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6617 - accuracy: 0.6034 - val_loss: 0.7101 - val_accuracy: 0.5025\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6604 - accuracy: 0.6058 - val_loss: 0.7124 - val_accuracy: 0.5188\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6605 - accuracy: 0.5952 - val_loss: 0.7127 - val_accuracy: 0.5000\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6597 - accuracy: 0.6112 - val_loss: 0.7123 - val_accuracy: 0.4981\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6606 - accuracy: 0.6108 - val_loss: 0.7138 - val_accuracy: 0.5188\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6571 - accuracy: 0.6077 - val_loss: 0.7144 - val_accuracy: 0.5094\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6529 - accuracy: 0.6211 - val_loss: 0.7192 - val_accuracy: 0.4938\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6504 - accuracy: 0.6273 - val_loss: 0.7265 - val_accuracy: 0.5038\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6521 - accuracy: 0.6167 - val_loss: 0.7192 - val_accuracy: 0.4981\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6511 - accuracy: 0.6166 - val_loss: 0.7214 - val_accuracy: 0.5025\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6500 - accuracy: 0.6264 - val_loss: 0.7266 - val_accuracy: 0.5019\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6506 - accuracy: 0.6180 - val_loss: 0.7201 - val_accuracy: 0.5013\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6506 - accuracy: 0.6162 - val_loss: 0.7235 - val_accuracy: 0.5019\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6467 - accuracy: 0.6223 - val_loss: 0.7196 - val_accuracy: 0.4850\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6492 - accuracy: 0.6222 - val_loss: 0.7222 - val_accuracy: 0.4981\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6462 - accuracy: 0.6202 - val_loss: 0.7233 - val_accuracy: 0.5019\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6438 - accuracy: 0.6267 - val_loss: 0.7254 - val_accuracy: 0.4969\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6453 - accuracy: 0.6230 - val_loss: 0.7260 - val_accuracy: 0.4969\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.6253 - val_loss: 0.7278 - val_accuracy: 0.4956\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6295 - val_loss: 0.7353 - val_accuracy: 0.5025\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6425 - accuracy: 0.6255 - val_loss: 0.7227 - val_accuracy: 0.4988\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6410 - accuracy: 0.6323 - val_loss: 0.7279 - val_accuracy: 0.4900\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6383 - accuracy: 0.6344 - val_loss: 0.7262 - val_accuracy: 0.5106\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6374 - accuracy: 0.6372 - val_loss: 0.7381 - val_accuracy: 0.5019\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6370 - accuracy: 0.6386 - val_loss: 0.7271 - val_accuracy: 0.5006\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6355 - accuracy: 0.6350 - val_loss: 0.7317 - val_accuracy: 0.5050\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6292 - accuracy: 0.6416 - val_loss: 0.7331 - val_accuracy: 0.5075\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6309 - accuracy: 0.6488 - val_loss: 0.7271 - val_accuracy: 0.5119\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6375 - val_loss: 0.7353 - val_accuracy: 0.5188\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6332 - accuracy: 0.6375 - val_loss: 0.7376 - val_accuracy: 0.5038\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6265 - accuracy: 0.6466 - val_loss: 0.7333 - val_accuracy: 0.4994\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6325 - accuracy: 0.6372 - val_loss: 0.7325 - val_accuracy: 0.5000\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6296 - accuracy: 0.6473 - val_loss: 0.7398 - val_accuracy: 0.5069\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6239 - accuracy: 0.6473 - val_loss: 0.7449 - val_accuracy: 0.4956\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6298 - accuracy: 0.6425 - val_loss: 0.7401 - val_accuracy: 0.5063\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6259 - accuracy: 0.6528 - val_loss: 0.7419 - val_accuracy: 0.5075\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6238 - accuracy: 0.6441 - val_loss: 0.7371 - val_accuracy: 0.5188\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6217 - accuracy: 0.6506 - val_loss: 0.7455 - val_accuracy: 0.5125\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6221 - accuracy: 0.6459 - val_loss: 0.7407 - val_accuracy: 0.5138\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6282 - accuracy: 0.6470 - val_loss: 0.7382 - val_accuracy: 0.5131\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6175 - accuracy: 0.6639 - val_loss: 0.7353 - val_accuracy: 0.5050\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6184 - accuracy: 0.6520 - val_loss: 0.7373 - val_accuracy: 0.5194\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6176 - accuracy: 0.6586 - val_loss: 0.7374 - val_accuracy: 0.5038\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6145 - accuracy: 0.6603 - val_loss: 0.7380 - val_accuracy: 0.5169\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6194 - accuracy: 0.6520 - val_loss: 0.7429 - val_accuracy: 0.5063\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6152 - accuracy: 0.6552 - val_loss: 0.7385 - val_accuracy: 0.5175\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6132 - accuracy: 0.6595 - val_loss: 0.7429 - val_accuracy: 0.5094\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6183 - accuracy: 0.6584 - val_loss: 0.7376 - val_accuracy: 0.5069\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6143 - accuracy: 0.6586 - val_loss: 0.7378 - val_accuracy: 0.5044\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6107 - accuracy: 0.6670 - val_loss: 0.7442 - val_accuracy: 0.5125\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6138 - accuracy: 0.6559 - val_loss: 0.7413 - val_accuracy: 0.5094\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6127 - accuracy: 0.6614 - val_loss: 0.7447 - val_accuracy: 0.5088\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6104 - accuracy: 0.6589 - val_loss: 0.7569 - val_accuracy: 0.5013\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6031 - accuracy: 0.6622 - val_loss: 0.7466 - val_accuracy: 0.5088\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6148 - accuracy: 0.6591 - val_loss: 0.7461 - val_accuracy: 0.5156\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6076 - accuracy: 0.6648 - val_loss: 0.7487 - val_accuracy: 0.5125\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6089 - accuracy: 0.6620 - val_loss: 0.7546 - val_accuracy: 0.5138\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6078 - accuracy: 0.6706 - val_loss: 0.7463 - val_accuracy: 0.5150\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6061 - accuracy: 0.6625 - val_loss: 0.7505 - val_accuracy: 0.5069\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6008 - accuracy: 0.6734 - val_loss: 0.7507 - val_accuracy: 0.5088\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6064 - accuracy: 0.6734 - val_loss: 0.7484 - val_accuracy: 0.5106\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6020 - accuracy: 0.6686 - val_loss: 0.7538 - val_accuracy: 0.5019\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6078 - accuracy: 0.6670 - val_loss: 0.7541 - val_accuracy: 0.5138\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5965 - accuracy: 0.6802 - val_loss: 0.7530 - val_accuracy: 0.5150\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6057 - accuracy: 0.6655 - val_loss: 0.7507 - val_accuracy: 0.5081\n",
      "Epoch 93/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5990 - accuracy: 0.6742 - val_loss: 0.7523 - val_accuracy: 0.5056\n",
      "Epoch 94/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5999 - accuracy: 0.6759 - val_loss: 0.7583 - val_accuracy: 0.5075\n",
      "Epoch 95/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5984 - accuracy: 0.6675 - val_loss: 0.7540 - val_accuracy: 0.5113\n",
      "Epoch 96/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5909 - accuracy: 0.6866 - val_loss: 0.7558 - val_accuracy: 0.5081\n",
      "Epoch 97/100\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5982 - accuracy: 0.6786 - val_loss: 0.7498 - val_accuracy: 0.5213\n",
      "Epoch 98/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5916 - accuracy: 0.6862 - val_loss: 0.7545 - val_accuracy: 0.5188\n",
      "Epoch 99/100\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5979 - accuracy: 0.6769 - val_loss: 0.7567 - val_accuracy: 0.5125\n",
      "Epoch 100/100\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5993 - accuracy: 0.6777 - val_loss: 0.7591 - val_accuracy: 0.5131\n",
      "63/63 [==============================] - 0s 3ms/step\n",
      "\n",
      "--- Neural Network Model Performance Metrics ---\n",
      "Accuracy: 0.5025\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.45      0.47       989\n",
      "         1.0       0.51      0.55      0.53      1011\n",
      "\n",
      "    accuracy                           0.50      2000\n",
      "   macro avg       0.50      0.50      0.50      2000\n",
      "weighted avg       0.50      0.50      0.50      2000\n",
      "\n",
      "ROC AUC Score: 0.5007\n",
      "\n",
      "--- Regression Metrics on Predicted Probabilities ---\n",
      "R-squared (R2): -0.1257\n",
      "Mean Absolute Error (MAE): 0.4991\n",
      "Mean Squared Error (MSE): 0.2814\n",
      "Root Mean Squared Error (RMSE): 0.5305\n",
      "Mean Absolute Percentage Error (MAPE): 2550664115.95%\n",
      "Mean Squared Log Error (MSLE): 0.1377\n",
      "\n",
      "Model performance metrics saved to 'nn_performance_metrics.xlsx'\n",
      "\n",
      "Confusion matrix plot saved to 'confusion_matrix.svg'\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perform_neural_network_classification(\"../mapped_dataset_Normalized_version.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71a99d10-e330-4980-910a-043c59b86378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, classification_report,\n",
    "    roc_auc_score, r2_score, mean_squared_error, mean_absolute_error,\n",
    "    mean_absolute_percentage_error, mean_squared_log_error\n",
    ")\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def perform_neural_network_classification(csv_file_path):\n",
    "    \"\"\"\n",
    "    Performs neural network binary classification on a dataset, calculates\n",
    "    various classification and regression metrics, generates a confusion\n",
    "    matrix heatmap, and saves all metrics to an Excel file.\n",
    "\n",
    "    Args:\n",
    "        csv_file_path (str): The path to the CSV file. All columns except the last\n",
    "                             are treated as features (X), and the last column,\n",
    "                             which should contain 0s and 1s, is the target variable (y).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the dataset\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "        # Separate features (X) and target (y)\n",
    "        X = df.iloc[:, :-1]  # All columns except the last\n",
    "        y = df.iloc[:, -1]   # The last column (0 or 1)\n",
    "\n",
    "        # Split the data into training and testing sets (80/20 split)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Standardize the data to help the neural network converge faster\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # --- Neural Network Model Setup ---\n",
    "        # Define a more complex sequential neural network model\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Input layer and first hidden layer with 128 neurons and ReLU activation\n",
    "        model.add(Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "        \n",
    "        # Dropout layer to prevent overfitting\n",
    "        model.add(Dropout(0.4))\n",
    "        \n",
    "        # Second hidden layer with 64 neurons and ReLU activation\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        \n",
    "        # Output layer for binary classification with a single neuron and sigmoid activation\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        # Compile the model with the Adam optimizer and binary cross-entropy loss\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Print the model summary\n",
    "        print(\"Model Summary:\")\n",
    "        model.summary()\n",
    "        \n",
    "        # Train the model with more epochs\n",
    "        print(\"\\nTraining Neural Network model...\")\n",
    "        history = model.fit(\n",
    "            X_train_scaled, y_train,\n",
    "            epochs=1000,  # Increased number of training epochs\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,  # Use 20% of the training data for validation\n",
    "            verbose=1  # Show training progress\n",
    "        )\n",
    "        \n",
    "        # --- Save the Trained Model ---\n",
    "        # Save the entire model (architecture, weights, and optimizer state)\n",
    "        model_path = 'best_model.keras'\n",
    "        model.save(model_path)\n",
    "        print(f\"\\nModel saved successfully to '{model_path}'\")\n",
    "\n",
    "        # --- Make Predictions ---\n",
    "        # The model predicts a probability. We round it to get a binary class (0 or 1).\n",
    "        y_pred_proba = model.predict(X_test_scaled).flatten()\n",
    "        y_pred = (y_pred_proba > 0.5).astype(\"int32\")\n",
    "\n",
    "        # --- 1. Calculate Classification Metrics ---\n",
    "        print(\"\\n--- Neural Network Model Performance Metrics ---\")\n",
    "\n",
    "        # Accuracy Score\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # Classification Report (Precision, Recall, F1-Score)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        # ROC AUC Score\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "        \n",
    "        # --- 2. Calculate Regression Metrics on Probabilities ---\n",
    "        print(\"\\n--- Regression Metrics on Predicted Probabilities ---\")\n",
    "\n",
    "        # R-squared (Coefficient of Determination)\n",
    "        r2 = r2_score(y_test, y_pred_proba)\n",
    "        print(f\"R-squared (R2): {r2:.4f}\")\n",
    "\n",
    "        # Mean Absolute Error (MAE)\n",
    "        mae = mean_absolute_error(y_test, y_pred_proba)\n",
    "        print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "\n",
    "        # Mean Squared Error (MSE)\n",
    "        mse = mean_squared_error(y_test, y_pred_proba)\n",
    "        print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "\n",
    "        # Root Mean Squared Error (RMSE)\n",
    "        rmse = np.sqrt(mse)\n",
    "        print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "\n",
    "        # Mean Absolute Percentage Error (MAPE)\n",
    "        # Add a small epsilon to avoid division by zero\n",
    "        mape = np.mean(np.abs((y_test - y_pred_proba) / (y_test + 1e-8))) * 100\n",
    "        print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "\n",
    "        # Mean Squared Log Error (MSLE) - check for negative values\n",
    "        # Add a small value to predictions to avoid log(0)\n",
    "        msle = mean_squared_log_error(y_test + 1e-8, y_pred_proba + 1e-8)\n",
    "        print(f\"Mean Squared Log Error (MSLE): {msle:.4f}\")\n",
    "\n",
    "        # --- 3. Save Metrics to Excel ---\n",
    "        # Create a dictionary to hold the metrics\n",
    "        metrics_data = {\n",
    "            'Metric': ['Accuracy', 'ROC AUC', 'R2 Score', 'MAE', 'MSE', 'RMSE', 'MAPE', 'MSLE'],\n",
    "            'Value': [accuracy, roc_auc, r2, mae, mse, rmse, mape, msle]\n",
    "        }\n",
    "        \n",
    "        metrics_df = pd.DataFrame(metrics_data)\n",
    "        excel_path = 'nn_performance_metrics.xlsx'\n",
    "        metrics_df.to_excel(excel_path, index=False)\n",
    "        print(f\"\\nModel performance metrics saved to '{excel_path}'\")\n",
    "\n",
    "        # --- 4. Generate Confusion Matrix Plot ---\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        plt.figure(figsize=(8, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                    xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "                    yticklabels=['Actual 0', 'Actual 1'])\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        \n",
    "        plot_path = 'confusion_matrix.svg'\n",
    "        plt.savefig(plot_path, format='svg')\n",
    "        print(f\"\\nConfusion matrix plot saved to '{plot_path}'\")\n",
    "        plt.show()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{csv_file_path}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d524fe34-6f70-4861-b978-32f5a280fbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary:\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 128)               1792      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10113 (39.50 KB)\n",
      "Trainable params: 10113 (39.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "Training Neural Network model...\n",
      "Epoch 1/1000\n",
      "200/200 [==============================] - 3s 5ms/step - loss: 0.7042 - accuracy: 0.5028 - val_loss: 0.6991 - val_accuracy: 0.5013\n",
      "Epoch 2/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6954 - accuracy: 0.5205 - val_loss: 0.6944 - val_accuracy: 0.5069\n",
      "Epoch 3/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6912 - accuracy: 0.5291 - val_loss: 0.6952 - val_accuracy: 0.5150\n",
      "Epoch 4/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6895 - accuracy: 0.5281 - val_loss: 0.6987 - val_accuracy: 0.5075\n",
      "Epoch 5/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6879 - accuracy: 0.5428 - val_loss: 0.6945 - val_accuracy: 0.5144\n",
      "Epoch 6/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6871 - accuracy: 0.5456 - val_loss: 0.6958 - val_accuracy: 0.5138\n",
      "Epoch 7/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6867 - accuracy: 0.5448 - val_loss: 0.6963 - val_accuracy: 0.5119\n",
      "Epoch 8/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6864 - accuracy: 0.5500 - val_loss: 0.6962 - val_accuracy: 0.5019\n",
      "Epoch 9/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6841 - accuracy: 0.5555 - val_loss: 0.6968 - val_accuracy: 0.4944\n",
      "Epoch 10/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6837 - accuracy: 0.5589 - val_loss: 0.6962 - val_accuracy: 0.5069\n",
      "Epoch 11/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6824 - accuracy: 0.5536 - val_loss: 0.6974 - val_accuracy: 0.5031\n",
      "Epoch 12/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6774 - accuracy: 0.5714 - val_loss: 0.6983 - val_accuracy: 0.5094\n",
      "Epoch 13/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6808 - accuracy: 0.5625 - val_loss: 0.6989 - val_accuracy: 0.5081\n",
      "Epoch 14/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6770 - accuracy: 0.5689 - val_loss: 0.7000 - val_accuracy: 0.4988\n",
      "Epoch 15/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6762 - accuracy: 0.5739 - val_loss: 0.7017 - val_accuracy: 0.5044\n",
      "Epoch 16/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6764 - accuracy: 0.5769 - val_loss: 0.6996 - val_accuracy: 0.5181\n",
      "Epoch 17/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6747 - accuracy: 0.5742 - val_loss: 0.7072 - val_accuracy: 0.5019\n",
      "Epoch 18/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6751 - accuracy: 0.5717 - val_loss: 0.7049 - val_accuracy: 0.5056\n",
      "Epoch 19/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6723 - accuracy: 0.5859 - val_loss: 0.7009 - val_accuracy: 0.5213\n",
      "Epoch 20/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6710 - accuracy: 0.5861 - val_loss: 0.7048 - val_accuracy: 0.5119\n",
      "Epoch 21/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6674 - accuracy: 0.5966 - val_loss: 0.7102 - val_accuracy: 0.5119\n",
      "Epoch 22/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6715 - accuracy: 0.5759 - val_loss: 0.7116 - val_accuracy: 0.5075\n",
      "Epoch 23/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6699 - accuracy: 0.5859 - val_loss: 0.7061 - val_accuracy: 0.5181\n",
      "Epoch 24/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6655 - accuracy: 0.5905 - val_loss: 0.7086 - val_accuracy: 0.5081\n",
      "Epoch 25/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6652 - accuracy: 0.5866 - val_loss: 0.7091 - val_accuracy: 0.5100\n",
      "Epoch 26/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6640 - accuracy: 0.6062 - val_loss: 0.7111 - val_accuracy: 0.5038\n",
      "Epoch 27/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6599 - accuracy: 0.5994 - val_loss: 0.7121 - val_accuracy: 0.5125\n",
      "Epoch 28/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6600 - accuracy: 0.5983 - val_loss: 0.7127 - val_accuracy: 0.5081\n",
      "Epoch 29/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6586 - accuracy: 0.6020 - val_loss: 0.7152 - val_accuracy: 0.5075\n",
      "Epoch 30/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6581 - accuracy: 0.6025 - val_loss: 0.7133 - val_accuracy: 0.5150\n",
      "Epoch 31/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6565 - accuracy: 0.6067 - val_loss: 0.7140 - val_accuracy: 0.5088\n",
      "Epoch 32/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6130 - val_loss: 0.7193 - val_accuracy: 0.5056\n",
      "Epoch 33/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6574 - accuracy: 0.6084 - val_loss: 0.7183 - val_accuracy: 0.5150\n",
      "Epoch 34/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6553 - accuracy: 0.6059 - val_loss: 0.7223 - val_accuracy: 0.5013\n",
      "Epoch 35/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6530 - accuracy: 0.6150 - val_loss: 0.7253 - val_accuracy: 0.5013\n",
      "Epoch 36/1000\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.6479 - accuracy: 0.6172 - val_loss: 0.7211 - val_accuracy: 0.5163\n",
      "Epoch 37/1000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.6504 - accuracy: 0.6178 - val_loss: 0.7228 - val_accuracy: 0.5100\n",
      "Epoch 38/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6504 - accuracy: 0.6222 - val_loss: 0.7226 - val_accuracy: 0.5138\n",
      "Epoch 39/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6499 - accuracy: 0.6197 - val_loss: 0.7256 - val_accuracy: 0.5063\n",
      "Epoch 40/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6474 - accuracy: 0.6206 - val_loss: 0.7234 - val_accuracy: 0.5025\n",
      "Epoch 41/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6474 - accuracy: 0.6209 - val_loss: 0.7234 - val_accuracy: 0.5150\n",
      "Epoch 42/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6443 - accuracy: 0.6231 - val_loss: 0.7252 - val_accuracy: 0.5094\n",
      "Epoch 43/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6449 - accuracy: 0.6203 - val_loss: 0.7277 - val_accuracy: 0.5100\n",
      "Epoch 44/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6443 - accuracy: 0.6263 - val_loss: 0.7307 - val_accuracy: 0.5056\n",
      "Epoch 45/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6404 - accuracy: 0.6275 - val_loss: 0.7352 - val_accuracy: 0.5156\n",
      "Epoch 46/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6394 - accuracy: 0.6297 - val_loss: 0.7360 - val_accuracy: 0.4988\n",
      "Epoch 47/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6302 - val_loss: 0.7303 - val_accuracy: 0.5081\n",
      "Epoch 48/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6219 - val_loss: 0.7355 - val_accuracy: 0.5063\n",
      "Epoch 49/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6420 - accuracy: 0.6316 - val_loss: 0.7327 - val_accuracy: 0.5031\n",
      "Epoch 50/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6391 - accuracy: 0.6273 - val_loss: 0.7345 - val_accuracy: 0.4944\n",
      "Epoch 51/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6342 - accuracy: 0.6402 - val_loss: 0.7349 - val_accuracy: 0.5181\n",
      "Epoch 52/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6368 - accuracy: 0.6258 - val_loss: 0.7362 - val_accuracy: 0.4913\n",
      "Epoch 53/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6321 - accuracy: 0.6461 - val_loss: 0.7380 - val_accuracy: 0.5081\n",
      "Epoch 54/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6333 - accuracy: 0.6363 - val_loss: 0.7392 - val_accuracy: 0.4988\n",
      "Epoch 55/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6319 - accuracy: 0.6441 - val_loss: 0.7423 - val_accuracy: 0.5025\n",
      "Epoch 56/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6333 - accuracy: 0.6361 - val_loss: 0.7473 - val_accuracy: 0.4956\n",
      "Epoch 57/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6333 - accuracy: 0.6367 - val_loss: 0.7420 - val_accuracy: 0.5069\n",
      "Epoch 58/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6339 - accuracy: 0.6391 - val_loss: 0.7432 - val_accuracy: 0.5119\n",
      "Epoch 59/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6289 - accuracy: 0.6455 - val_loss: 0.7488 - val_accuracy: 0.5000\n",
      "Epoch 60/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6303 - accuracy: 0.6380 - val_loss: 0.7451 - val_accuracy: 0.5006\n",
      "Epoch 61/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6292 - accuracy: 0.6398 - val_loss: 0.7478 - val_accuracy: 0.4913\n",
      "Epoch 62/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6272 - accuracy: 0.6438 - val_loss: 0.7474 - val_accuracy: 0.4950\n",
      "Epoch 63/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6216 - accuracy: 0.6523 - val_loss: 0.7454 - val_accuracy: 0.5000\n",
      "Epoch 64/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6226 - accuracy: 0.6525 - val_loss: 0.7455 - val_accuracy: 0.5000\n",
      "Epoch 65/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6447 - val_loss: 0.7465 - val_accuracy: 0.5050\n",
      "Epoch 66/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6223 - accuracy: 0.6505 - val_loss: 0.7470 - val_accuracy: 0.4994\n",
      "Epoch 67/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6192 - accuracy: 0.6550 - val_loss: 0.7561 - val_accuracy: 0.4931\n",
      "Epoch 68/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6177 - accuracy: 0.6575 - val_loss: 0.7542 - val_accuracy: 0.5031\n",
      "Epoch 69/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6205 - accuracy: 0.6486 - val_loss: 0.7566 - val_accuracy: 0.5100\n",
      "Epoch 70/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6188 - accuracy: 0.6569 - val_loss: 0.7616 - val_accuracy: 0.5000\n",
      "Epoch 71/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6163 - accuracy: 0.6592 - val_loss: 0.7576 - val_accuracy: 0.5094\n",
      "Epoch 72/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6080 - accuracy: 0.6620 - val_loss: 0.7596 - val_accuracy: 0.4988\n",
      "Epoch 73/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6176 - accuracy: 0.6484 - val_loss: 0.7620 - val_accuracy: 0.4944\n",
      "Epoch 74/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6153 - accuracy: 0.6605 - val_loss: 0.7593 - val_accuracy: 0.4913\n",
      "Epoch 75/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6133 - accuracy: 0.6642 - val_loss: 0.7590 - val_accuracy: 0.4944\n",
      "Epoch 76/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6153 - accuracy: 0.6592 - val_loss: 0.7644 - val_accuracy: 0.5025\n",
      "Epoch 77/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6091 - accuracy: 0.6612 - val_loss: 0.7634 - val_accuracy: 0.4969\n",
      "Epoch 78/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6143 - accuracy: 0.6495 - val_loss: 0.7614 - val_accuracy: 0.4894\n",
      "Epoch 79/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6099 - accuracy: 0.6634 - val_loss: 0.7641 - val_accuracy: 0.5031\n",
      "Epoch 80/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6143 - accuracy: 0.6617 - val_loss: 0.7652 - val_accuracy: 0.4994\n",
      "Epoch 81/1000\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.6077 - accuracy: 0.6656 - val_loss: 0.7676 - val_accuracy: 0.4981\n",
      "Epoch 82/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6019 - accuracy: 0.6689 - val_loss: 0.7626 - val_accuracy: 0.5006\n",
      "Epoch 83/1000\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.6101 - accuracy: 0.6595 - val_loss: 0.7665 - val_accuracy: 0.4988\n",
      "Epoch 84/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6027 - accuracy: 0.6727 - val_loss: 0.7638 - val_accuracy: 0.5050\n",
      "Epoch 85/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6066 - accuracy: 0.6708 - val_loss: 0.7597 - val_accuracy: 0.5144\n",
      "Epoch 86/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6068 - accuracy: 0.6639 - val_loss: 0.7592 - val_accuracy: 0.5094\n",
      "Epoch 87/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6065 - accuracy: 0.6648 - val_loss: 0.7653 - val_accuracy: 0.5075\n",
      "Epoch 88/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6067 - accuracy: 0.6609 - val_loss: 0.7674 - val_accuracy: 0.4981\n",
      "Epoch 89/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6027 - accuracy: 0.6703 - val_loss: 0.7677 - val_accuracy: 0.5063\n",
      "Epoch 90/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6085 - accuracy: 0.6612 - val_loss: 0.7700 - val_accuracy: 0.5025\n",
      "Epoch 91/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6010 - accuracy: 0.6681 - val_loss: 0.7648 - val_accuracy: 0.5038\n",
      "Epoch 92/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6062 - accuracy: 0.6659 - val_loss: 0.7676 - val_accuracy: 0.5094\n",
      "Epoch 93/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5978 - accuracy: 0.6731 - val_loss: 0.7717 - val_accuracy: 0.5081\n",
      "Epoch 94/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5998 - accuracy: 0.6695 - val_loss: 0.7789 - val_accuracy: 0.4981\n",
      "Epoch 95/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6001 - accuracy: 0.6727 - val_loss: 0.7741 - val_accuracy: 0.4975\n",
      "Epoch 96/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5975 - accuracy: 0.6723 - val_loss: 0.7750 - val_accuracy: 0.5069\n",
      "Epoch 97/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5976 - accuracy: 0.6758 - val_loss: 0.7789 - val_accuracy: 0.4919\n",
      "Epoch 98/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5970 - accuracy: 0.6748 - val_loss: 0.7877 - val_accuracy: 0.4850\n",
      "Epoch 99/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5938 - accuracy: 0.6791 - val_loss: 0.7776 - val_accuracy: 0.5000\n",
      "Epoch 100/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5883 - accuracy: 0.6836 - val_loss: 0.7841 - val_accuracy: 0.4925\n",
      "Epoch 101/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5928 - accuracy: 0.6806 - val_loss: 0.7784 - val_accuracy: 0.4988\n",
      "Epoch 102/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5934 - accuracy: 0.6781 - val_loss: 0.7698 - val_accuracy: 0.5075\n",
      "Epoch 103/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5904 - accuracy: 0.6872 - val_loss: 0.7756 - val_accuracy: 0.4925\n",
      "Epoch 104/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5875 - accuracy: 0.6834 - val_loss: 0.7848 - val_accuracy: 0.4894\n",
      "Epoch 105/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5912 - accuracy: 0.6795 - val_loss: 0.7809 - val_accuracy: 0.4981\n",
      "Epoch 106/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5907 - accuracy: 0.6858 - val_loss: 0.7810 - val_accuracy: 0.5038\n",
      "Epoch 107/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5896 - accuracy: 0.6833 - val_loss: 0.7864 - val_accuracy: 0.5006\n",
      "Epoch 108/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5896 - accuracy: 0.6789 - val_loss: 0.7805 - val_accuracy: 0.5088\n",
      "Epoch 109/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5912 - accuracy: 0.6823 - val_loss: 0.7841 - val_accuracy: 0.4994\n",
      "Epoch 110/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5817 - accuracy: 0.6823 - val_loss: 0.7805 - val_accuracy: 0.5050\n",
      "Epoch 111/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5858 - accuracy: 0.6911 - val_loss: 0.7963 - val_accuracy: 0.5056\n",
      "Epoch 112/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5832 - accuracy: 0.6855 - val_loss: 0.7949 - val_accuracy: 0.4981\n",
      "Epoch 113/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5757 - accuracy: 0.6942 - val_loss: 0.7949 - val_accuracy: 0.4963\n",
      "Epoch 114/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5765 - accuracy: 0.6923 - val_loss: 0.7862 - val_accuracy: 0.5025\n",
      "Epoch 115/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5885 - accuracy: 0.6789 - val_loss: 0.8001 - val_accuracy: 0.4969\n",
      "Epoch 116/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5856 - accuracy: 0.6811 - val_loss: 0.7878 - val_accuracy: 0.4975\n",
      "Epoch 117/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5832 - accuracy: 0.6913 - val_loss: 0.7974 - val_accuracy: 0.4931\n",
      "Epoch 118/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5800 - accuracy: 0.6888 - val_loss: 0.7876 - val_accuracy: 0.5013\n",
      "Epoch 119/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5770 - accuracy: 0.6880 - val_loss: 0.8021 - val_accuracy: 0.5025\n",
      "Epoch 120/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5768 - accuracy: 0.6883 - val_loss: 0.7978 - val_accuracy: 0.4975\n",
      "Epoch 121/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5811 - accuracy: 0.6933 - val_loss: 0.8012 - val_accuracy: 0.4956\n",
      "Epoch 122/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5769 - accuracy: 0.6894 - val_loss: 0.7952 - val_accuracy: 0.4963\n",
      "Epoch 123/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5776 - accuracy: 0.6934 - val_loss: 0.7932 - val_accuracy: 0.5113\n",
      "Epoch 124/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5759 - accuracy: 0.6984 - val_loss: 0.7973 - val_accuracy: 0.5106\n",
      "Epoch 125/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5783 - accuracy: 0.6947 - val_loss: 0.7937 - val_accuracy: 0.5094\n",
      "Epoch 126/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5838 - accuracy: 0.6870 - val_loss: 0.7955 - val_accuracy: 0.4994\n",
      "Epoch 127/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5775 - accuracy: 0.6953 - val_loss: 0.7943 - val_accuracy: 0.4963\n",
      "Epoch 128/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5772 - accuracy: 0.6867 - val_loss: 0.7895 - val_accuracy: 0.5038\n",
      "Epoch 129/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5764 - accuracy: 0.6961 - val_loss: 0.8026 - val_accuracy: 0.5025\n",
      "Epoch 130/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5721 - accuracy: 0.7008 - val_loss: 0.8002 - val_accuracy: 0.5100\n",
      "Epoch 131/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5762 - accuracy: 0.6941 - val_loss: 0.7980 - val_accuracy: 0.5094\n",
      "Epoch 132/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5753 - accuracy: 0.6909 - val_loss: 0.7969 - val_accuracy: 0.4994\n",
      "Epoch 133/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5688 - accuracy: 0.6983 - val_loss: 0.8011 - val_accuracy: 0.5075\n",
      "Epoch 134/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5682 - accuracy: 0.6975 - val_loss: 0.8032 - val_accuracy: 0.5056\n",
      "Epoch 135/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5703 - accuracy: 0.7019 - val_loss: 0.8174 - val_accuracy: 0.4956\n",
      "Epoch 136/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5695 - accuracy: 0.7028 - val_loss: 0.8176 - val_accuracy: 0.4819\n",
      "Epoch 137/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5729 - accuracy: 0.6952 - val_loss: 0.8118 - val_accuracy: 0.5013\n",
      "Epoch 138/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5660 - accuracy: 0.7034 - val_loss: 0.8068 - val_accuracy: 0.5006\n",
      "Epoch 139/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5672 - accuracy: 0.6984 - val_loss: 0.8113 - val_accuracy: 0.4975\n",
      "Epoch 140/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5675 - accuracy: 0.6997 - val_loss: 0.8102 - val_accuracy: 0.4969\n",
      "Epoch 141/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5652 - accuracy: 0.7059 - val_loss: 0.8209 - val_accuracy: 0.4925\n",
      "Epoch 142/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5718 - accuracy: 0.6963 - val_loss: 0.8090 - val_accuracy: 0.4988\n",
      "Epoch 143/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5649 - accuracy: 0.7022 - val_loss: 0.8110 - val_accuracy: 0.5069\n",
      "Epoch 144/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5718 - accuracy: 0.6947 - val_loss: 0.8127 - val_accuracy: 0.5044\n",
      "Epoch 145/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5650 - accuracy: 0.6998 - val_loss: 0.8120 - val_accuracy: 0.5119\n",
      "Epoch 146/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5622 - accuracy: 0.7084 - val_loss: 0.8134 - val_accuracy: 0.5031\n",
      "Epoch 147/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5654 - accuracy: 0.7084 - val_loss: 0.8095 - val_accuracy: 0.5006\n",
      "Epoch 148/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5631 - accuracy: 0.7067 - val_loss: 0.8132 - val_accuracy: 0.4975\n",
      "Epoch 149/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5649 - accuracy: 0.7028 - val_loss: 0.8109 - val_accuracy: 0.4994\n",
      "Epoch 150/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5632 - accuracy: 0.7042 - val_loss: 0.8202 - val_accuracy: 0.5063\n",
      "Epoch 151/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5566 - accuracy: 0.7077 - val_loss: 0.8238 - val_accuracy: 0.4950\n",
      "Epoch 152/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5678 - accuracy: 0.7023 - val_loss: 0.8157 - val_accuracy: 0.5013\n",
      "Epoch 153/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5575 - accuracy: 0.7094 - val_loss: 0.8166 - val_accuracy: 0.5000\n",
      "Epoch 154/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5586 - accuracy: 0.7119 - val_loss: 0.8234 - val_accuracy: 0.4969\n",
      "Epoch 155/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5613 - accuracy: 0.7063 - val_loss: 0.8264 - val_accuracy: 0.5100\n",
      "Epoch 156/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5592 - accuracy: 0.7134 - val_loss: 0.8291 - val_accuracy: 0.5056\n",
      "Epoch 157/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5591 - accuracy: 0.7047 - val_loss: 0.8215 - val_accuracy: 0.4994\n",
      "Epoch 158/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5570 - accuracy: 0.7098 - val_loss: 0.8225 - val_accuracy: 0.4969\n",
      "Epoch 159/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5563 - accuracy: 0.7047 - val_loss: 0.8256 - val_accuracy: 0.4900\n",
      "Epoch 160/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5647 - accuracy: 0.6980 - val_loss: 0.8266 - val_accuracy: 0.5038\n",
      "Epoch 161/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5513 - accuracy: 0.7097 - val_loss: 0.8287 - val_accuracy: 0.4944\n",
      "Epoch 162/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5492 - accuracy: 0.7109 - val_loss: 0.8267 - val_accuracy: 0.4931\n",
      "Epoch 163/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5499 - accuracy: 0.7130 - val_loss: 0.8464 - val_accuracy: 0.4938\n",
      "Epoch 164/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5587 - accuracy: 0.7089 - val_loss: 0.8369 - val_accuracy: 0.4850\n",
      "Epoch 165/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5537 - accuracy: 0.7147 - val_loss: 0.8251 - val_accuracy: 0.4975\n",
      "Epoch 166/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5555 - accuracy: 0.7100 - val_loss: 0.8279 - val_accuracy: 0.5050\n",
      "Epoch 167/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5525 - accuracy: 0.7086 - val_loss: 0.8276 - val_accuracy: 0.5063\n",
      "Epoch 168/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5616 - accuracy: 0.7042 - val_loss: 0.8286 - val_accuracy: 0.5025\n",
      "Epoch 169/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5445 - accuracy: 0.7170 - val_loss: 0.8313 - val_accuracy: 0.5094\n",
      "Epoch 170/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5556 - accuracy: 0.7095 - val_loss: 0.8316 - val_accuracy: 0.4950\n",
      "Epoch 171/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5515 - accuracy: 0.7156 - val_loss: 0.8428 - val_accuracy: 0.5038\n",
      "Epoch 172/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5398 - accuracy: 0.7266 - val_loss: 0.8367 - val_accuracy: 0.5019\n",
      "Epoch 173/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5359 - accuracy: 0.7294 - val_loss: 0.8287 - val_accuracy: 0.5013\n",
      "Epoch 174/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5474 - accuracy: 0.7177 - val_loss: 0.8403 - val_accuracy: 0.5088\n",
      "Epoch 175/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5520 - accuracy: 0.7136 - val_loss: 0.8335 - val_accuracy: 0.5069\n",
      "Epoch 176/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5443 - accuracy: 0.7209 - val_loss: 0.8267 - val_accuracy: 0.5056\n",
      "Epoch 177/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5487 - accuracy: 0.7131 - val_loss: 0.8348 - val_accuracy: 0.5019\n",
      "Epoch 178/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5423 - accuracy: 0.7225 - val_loss: 0.8313 - val_accuracy: 0.5044\n",
      "Epoch 179/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5466 - accuracy: 0.7177 - val_loss: 0.8441 - val_accuracy: 0.4981\n",
      "Epoch 180/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5511 - accuracy: 0.7155 - val_loss: 0.8445 - val_accuracy: 0.4969\n",
      "Epoch 181/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5317 - accuracy: 0.7319 - val_loss: 0.8382 - val_accuracy: 0.5106\n",
      "Epoch 182/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5469 - accuracy: 0.7161 - val_loss: 0.8384 - val_accuracy: 0.5100\n",
      "Epoch 183/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5424 - accuracy: 0.7145 - val_loss: 0.8376 - val_accuracy: 0.5050\n",
      "Epoch 184/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5421 - accuracy: 0.7209 - val_loss: 0.8366 - val_accuracy: 0.5106\n",
      "Epoch 185/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5413 - accuracy: 0.7197 - val_loss: 0.8498 - val_accuracy: 0.5019\n",
      "Epoch 186/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5376 - accuracy: 0.7197 - val_loss: 0.8433 - val_accuracy: 0.4963\n",
      "Epoch 187/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5476 - accuracy: 0.7202 - val_loss: 0.8421 - val_accuracy: 0.4988\n",
      "Epoch 188/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5392 - accuracy: 0.7236 - val_loss: 0.8450 - val_accuracy: 0.5000\n",
      "Epoch 189/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5407 - accuracy: 0.7228 - val_loss: 0.8473 - val_accuracy: 0.4981\n",
      "Epoch 190/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5436 - accuracy: 0.7175 - val_loss: 0.8459 - val_accuracy: 0.4956\n",
      "Epoch 191/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5351 - accuracy: 0.7295 - val_loss: 0.8561 - val_accuracy: 0.4975\n",
      "Epoch 192/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5385 - accuracy: 0.7214 - val_loss: 0.8429 - val_accuracy: 0.4969\n",
      "Epoch 193/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5454 - accuracy: 0.7222 - val_loss: 0.8408 - val_accuracy: 0.4963\n",
      "Epoch 194/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5375 - accuracy: 0.7295 - val_loss: 0.8433 - val_accuracy: 0.4956\n",
      "Epoch 195/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5376 - accuracy: 0.7198 - val_loss: 0.8475 - val_accuracy: 0.5050\n",
      "Epoch 196/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5454 - accuracy: 0.7153 - val_loss: 0.8487 - val_accuracy: 0.4875\n",
      "Epoch 197/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5414 - accuracy: 0.7273 - val_loss: 0.8471 - val_accuracy: 0.4981\n",
      "Epoch 198/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5434 - accuracy: 0.7225 - val_loss: 0.8463 - val_accuracy: 0.4931\n",
      "Epoch 199/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5449 - accuracy: 0.7172 - val_loss: 0.8442 - val_accuracy: 0.5006\n",
      "Epoch 200/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5299 - accuracy: 0.7262 - val_loss: 0.8538 - val_accuracy: 0.4906\n",
      "Epoch 201/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5280 - accuracy: 0.7333 - val_loss: 0.8607 - val_accuracy: 0.4881\n",
      "Epoch 202/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5357 - accuracy: 0.7253 - val_loss: 0.8616 - val_accuracy: 0.5019\n",
      "Epoch 203/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5388 - accuracy: 0.7222 - val_loss: 0.8690 - val_accuracy: 0.4919\n",
      "Epoch 204/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5404 - accuracy: 0.7237 - val_loss: 0.8572 - val_accuracy: 0.4919\n",
      "Epoch 205/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5382 - accuracy: 0.7222 - val_loss: 0.8576 - val_accuracy: 0.4913\n",
      "Epoch 206/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5401 - accuracy: 0.7259 - val_loss: 0.8596 - val_accuracy: 0.4975\n",
      "Epoch 207/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5289 - accuracy: 0.7309 - val_loss: 0.8596 - val_accuracy: 0.4994\n",
      "Epoch 208/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5351 - accuracy: 0.7270 - val_loss: 0.8660 - val_accuracy: 0.5031\n",
      "Epoch 209/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5320 - accuracy: 0.7258 - val_loss: 0.8573 - val_accuracy: 0.5031\n",
      "Epoch 210/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5320 - accuracy: 0.7256 - val_loss: 0.8523 - val_accuracy: 0.5050\n",
      "Epoch 211/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5327 - accuracy: 0.7303 - val_loss: 0.8551 - val_accuracy: 0.5000\n",
      "Epoch 212/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5348 - accuracy: 0.7312 - val_loss: 0.8587 - val_accuracy: 0.4956\n",
      "Epoch 213/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5300 - accuracy: 0.7289 - val_loss: 0.8685 - val_accuracy: 0.4906\n",
      "Epoch 214/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5328 - accuracy: 0.7267 - val_loss: 0.8629 - val_accuracy: 0.5006\n",
      "Epoch 215/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5258 - accuracy: 0.7353 - val_loss: 0.8786 - val_accuracy: 0.4931\n",
      "Epoch 216/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5316 - accuracy: 0.7309 - val_loss: 0.8613 - val_accuracy: 0.4919\n",
      "Epoch 217/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5277 - accuracy: 0.7327 - val_loss: 0.8532 - val_accuracy: 0.5050\n",
      "Epoch 218/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5280 - accuracy: 0.7311 - val_loss: 0.8640 - val_accuracy: 0.5044\n",
      "Epoch 219/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5347 - accuracy: 0.7248 - val_loss: 0.8635 - val_accuracy: 0.5044\n",
      "Epoch 220/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5302 - accuracy: 0.7305 - val_loss: 0.8631 - val_accuracy: 0.4969\n",
      "Epoch 221/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5262 - accuracy: 0.7380 - val_loss: 0.8577 - val_accuracy: 0.5031\n",
      "Epoch 222/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5310 - accuracy: 0.7320 - val_loss: 0.8605 - val_accuracy: 0.4969\n",
      "Epoch 223/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5195 - accuracy: 0.7416 - val_loss: 0.8688 - val_accuracy: 0.4969\n",
      "Epoch 224/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5246 - accuracy: 0.7369 - val_loss: 0.8700 - val_accuracy: 0.4938\n",
      "Epoch 225/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5270 - accuracy: 0.7337 - val_loss: 0.8690 - val_accuracy: 0.4950\n",
      "Epoch 226/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5254 - accuracy: 0.7336 - val_loss: 0.8741 - val_accuracy: 0.4975\n",
      "Epoch 227/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5267 - accuracy: 0.7319 - val_loss: 0.8736 - val_accuracy: 0.4875\n",
      "Epoch 228/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5224 - accuracy: 0.7361 - val_loss: 0.8676 - val_accuracy: 0.4881\n",
      "Epoch 229/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5262 - accuracy: 0.7334 - val_loss: 0.8742 - val_accuracy: 0.4875\n",
      "Epoch 230/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5227 - accuracy: 0.7383 - val_loss: 0.8748 - val_accuracy: 0.4869\n",
      "Epoch 231/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5278 - accuracy: 0.7272 - val_loss: 0.8783 - val_accuracy: 0.4881\n",
      "Epoch 232/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5316 - accuracy: 0.7283 - val_loss: 0.8759 - val_accuracy: 0.4900\n",
      "Epoch 233/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5234 - accuracy: 0.7320 - val_loss: 0.8669 - val_accuracy: 0.4938\n",
      "Epoch 234/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5225 - accuracy: 0.7345 - val_loss: 0.8776 - val_accuracy: 0.4906\n",
      "Epoch 235/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5256 - accuracy: 0.7348 - val_loss: 0.8698 - val_accuracy: 0.4975\n",
      "Epoch 236/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5216 - accuracy: 0.7384 - val_loss: 0.8773 - val_accuracy: 0.4863\n",
      "Epoch 237/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5221 - accuracy: 0.7428 - val_loss: 0.8697 - val_accuracy: 0.5019\n",
      "Epoch 238/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5277 - accuracy: 0.7259 - val_loss: 0.8781 - val_accuracy: 0.4950\n",
      "Epoch 239/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5268 - accuracy: 0.7295 - val_loss: 0.8740 - val_accuracy: 0.4975\n",
      "Epoch 240/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5335 - accuracy: 0.7270 - val_loss: 0.8737 - val_accuracy: 0.4863\n",
      "Epoch 241/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5302 - accuracy: 0.7289 - val_loss: 0.8801 - val_accuracy: 0.4944\n",
      "Epoch 242/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5242 - accuracy: 0.7272 - val_loss: 0.8882 - val_accuracy: 0.4913\n",
      "Epoch 243/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5235 - accuracy: 0.7372 - val_loss: 0.8699 - val_accuracy: 0.4931\n",
      "Epoch 244/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5232 - accuracy: 0.7375 - val_loss: 0.8893 - val_accuracy: 0.4975\n",
      "Epoch 245/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5251 - accuracy: 0.7323 - val_loss: 0.8760 - val_accuracy: 0.5000\n",
      "Epoch 246/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5246 - accuracy: 0.7336 - val_loss: 0.8720 - val_accuracy: 0.5044\n",
      "Epoch 247/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5187 - accuracy: 0.7437 - val_loss: 0.8732 - val_accuracy: 0.5013\n",
      "Epoch 248/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5137 - accuracy: 0.7430 - val_loss: 0.8808 - val_accuracy: 0.4956\n",
      "Epoch 249/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5263 - accuracy: 0.7380 - val_loss: 0.8804 - val_accuracy: 0.4919\n",
      "Epoch 250/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5229 - accuracy: 0.7297 - val_loss: 0.8730 - val_accuracy: 0.4956\n",
      "Epoch 251/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5168 - accuracy: 0.7400 - val_loss: 0.8912 - val_accuracy: 0.4894\n",
      "Epoch 252/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5319 - accuracy: 0.7306 - val_loss: 0.8775 - val_accuracy: 0.5000\n",
      "Epoch 253/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5240 - accuracy: 0.7378 - val_loss: 0.8779 - val_accuracy: 0.5069\n",
      "Epoch 254/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5159 - accuracy: 0.7423 - val_loss: 0.8787 - val_accuracy: 0.4988\n",
      "Epoch 255/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5184 - accuracy: 0.7350 - val_loss: 0.8816 - val_accuracy: 0.4963\n",
      "Epoch 256/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5280 - accuracy: 0.7337 - val_loss: 0.8878 - val_accuracy: 0.4988\n",
      "Epoch 257/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5246 - accuracy: 0.7328 - val_loss: 0.8813 - val_accuracy: 0.4969\n",
      "Epoch 258/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5204 - accuracy: 0.7394 - val_loss: 0.8871 - val_accuracy: 0.5000\n",
      "Epoch 259/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5224 - accuracy: 0.7384 - val_loss: 0.8802 - val_accuracy: 0.5050\n",
      "Epoch 260/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5223 - accuracy: 0.7330 - val_loss: 0.8897 - val_accuracy: 0.4969\n",
      "Epoch 261/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5214 - accuracy: 0.7378 - val_loss: 0.8889 - val_accuracy: 0.5119\n",
      "Epoch 262/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5115 - accuracy: 0.7412 - val_loss: 0.8832 - val_accuracy: 0.5100\n",
      "Epoch 263/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5091 - accuracy: 0.7447 - val_loss: 0.8932 - val_accuracy: 0.5081\n",
      "Epoch 264/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5290 - accuracy: 0.7281 - val_loss: 0.8984 - val_accuracy: 0.5025\n",
      "Epoch 265/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5145 - accuracy: 0.7434 - val_loss: 0.8944 - val_accuracy: 0.4956\n",
      "Epoch 266/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5219 - accuracy: 0.7377 - val_loss: 0.9005 - val_accuracy: 0.4963\n",
      "Epoch 267/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5171 - accuracy: 0.7347 - val_loss: 0.8935 - val_accuracy: 0.5013\n",
      "Epoch 268/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7505 - val_loss: 0.8946 - val_accuracy: 0.5044\n",
      "Epoch 269/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5166 - accuracy: 0.7403 - val_loss: 0.8949 - val_accuracy: 0.5081\n",
      "Epoch 270/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5173 - accuracy: 0.7370 - val_loss: 0.8896 - val_accuracy: 0.5119\n",
      "Epoch 271/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7417 - val_loss: 0.8910 - val_accuracy: 0.5094\n",
      "Epoch 272/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5192 - accuracy: 0.7394 - val_loss: 0.9004 - val_accuracy: 0.5000\n",
      "Epoch 273/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5125 - accuracy: 0.7511 - val_loss: 0.8952 - val_accuracy: 0.5038\n",
      "Epoch 274/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5121 - accuracy: 0.7466 - val_loss: 0.8958 - val_accuracy: 0.4938\n",
      "Epoch 275/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5054 - accuracy: 0.7517 - val_loss: 0.8995 - val_accuracy: 0.4975\n",
      "Epoch 276/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5164 - accuracy: 0.7378 - val_loss: 0.8851 - val_accuracy: 0.5056\n",
      "Epoch 277/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5125 - accuracy: 0.7419 - val_loss: 0.8930 - val_accuracy: 0.4881\n",
      "Epoch 278/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5049 - accuracy: 0.7430 - val_loss: 0.8950 - val_accuracy: 0.5019\n",
      "Epoch 279/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5164 - accuracy: 0.7394 - val_loss: 0.8901 - val_accuracy: 0.5119\n",
      "Epoch 280/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7461 - val_loss: 0.8894 - val_accuracy: 0.5069\n",
      "Epoch 281/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5222 - accuracy: 0.7383 - val_loss: 0.8929 - val_accuracy: 0.5094\n",
      "Epoch 282/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5207 - accuracy: 0.7397 - val_loss: 0.8923 - val_accuracy: 0.5019\n",
      "Epoch 283/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5089 - accuracy: 0.7466 - val_loss: 0.8936 - val_accuracy: 0.5038\n",
      "Epoch 284/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5075 - accuracy: 0.7491 - val_loss: 0.9000 - val_accuracy: 0.5019\n",
      "Epoch 285/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5204 - accuracy: 0.7369 - val_loss: 0.8928 - val_accuracy: 0.5075\n",
      "Epoch 286/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5061 - accuracy: 0.7494 - val_loss: 0.8946 - val_accuracy: 0.5063\n",
      "Epoch 287/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.7494 - val_loss: 0.8993 - val_accuracy: 0.5075\n",
      "Epoch 288/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5087 - accuracy: 0.7472 - val_loss: 0.8981 - val_accuracy: 0.4969\n",
      "Epoch 289/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5041 - accuracy: 0.7517 - val_loss: 0.9032 - val_accuracy: 0.5206\n",
      "Epoch 290/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5089 - accuracy: 0.7520 - val_loss: 0.8948 - val_accuracy: 0.5013\n",
      "Epoch 291/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5060 - accuracy: 0.7498 - val_loss: 0.8975 - val_accuracy: 0.5025\n",
      "Epoch 292/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7502 - val_loss: 0.8983 - val_accuracy: 0.5019\n",
      "Epoch 293/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5066 - accuracy: 0.7431 - val_loss: 0.9043 - val_accuracy: 0.5000\n",
      "Epoch 294/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5049 - accuracy: 0.7567 - val_loss: 0.8981 - val_accuracy: 0.4956\n",
      "Epoch 295/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5141 - accuracy: 0.7450 - val_loss: 0.9049 - val_accuracy: 0.4944\n",
      "Epoch 296/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5080 - accuracy: 0.7478 - val_loss: 0.9014 - val_accuracy: 0.5013\n",
      "Epoch 297/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5052 - accuracy: 0.7467 - val_loss: 0.9077 - val_accuracy: 0.4881\n",
      "Epoch 298/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5020 - accuracy: 0.7547 - val_loss: 0.8970 - val_accuracy: 0.5019\n",
      "Epoch 299/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4981 - accuracy: 0.7514 - val_loss: 0.9082 - val_accuracy: 0.5000\n",
      "Epoch 300/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5189 - accuracy: 0.7423 - val_loss: 0.9014 - val_accuracy: 0.5000\n",
      "Epoch 301/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5062 - accuracy: 0.7495 - val_loss: 0.9075 - val_accuracy: 0.4931\n",
      "Epoch 302/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5088 - accuracy: 0.7487 - val_loss: 0.9063 - val_accuracy: 0.5013\n",
      "Epoch 303/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5055 - accuracy: 0.7545 - val_loss: 0.9148 - val_accuracy: 0.4963\n",
      "Epoch 304/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4999 - accuracy: 0.7511 - val_loss: 0.9132 - val_accuracy: 0.5044\n",
      "Epoch 305/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4977 - accuracy: 0.7544 - val_loss: 0.9093 - val_accuracy: 0.5050\n",
      "Epoch 306/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5033 - accuracy: 0.7489 - val_loss: 0.9076 - val_accuracy: 0.5006\n",
      "Epoch 307/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5013 - accuracy: 0.7534 - val_loss: 0.9123 - val_accuracy: 0.5063\n",
      "Epoch 308/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5050 - accuracy: 0.7531 - val_loss: 0.9010 - val_accuracy: 0.5075\n",
      "Epoch 309/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5061 - accuracy: 0.7486 - val_loss: 0.9019 - val_accuracy: 0.5019\n",
      "Epoch 310/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5146 - accuracy: 0.7377 - val_loss: 0.9113 - val_accuracy: 0.4988\n",
      "Epoch 311/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5063 - accuracy: 0.7477 - val_loss: 0.9185 - val_accuracy: 0.4994\n",
      "Epoch 312/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7428 - val_loss: 0.9059 - val_accuracy: 0.5019\n",
      "Epoch 313/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5261 - accuracy: 0.7331 - val_loss: 0.9035 - val_accuracy: 0.4963\n",
      "Epoch 314/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5014 - accuracy: 0.7508 - val_loss: 0.9095 - val_accuracy: 0.4950\n",
      "Epoch 315/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5023 - accuracy: 0.7484 - val_loss: 0.9134 - val_accuracy: 0.4981\n",
      "Epoch 316/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4904 - accuracy: 0.7580 - val_loss: 0.9188 - val_accuracy: 0.4906\n",
      "Epoch 317/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4936 - accuracy: 0.7581 - val_loss: 0.9115 - val_accuracy: 0.4956\n",
      "Epoch 318/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7436 - val_loss: 0.9203 - val_accuracy: 0.4913\n",
      "Epoch 319/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4996 - accuracy: 0.7516 - val_loss: 0.9136 - val_accuracy: 0.4981\n",
      "Epoch 320/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5043 - accuracy: 0.7505 - val_loss: 0.9138 - val_accuracy: 0.4994\n",
      "Epoch 321/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5001 - accuracy: 0.7520 - val_loss: 0.9140 - val_accuracy: 0.5088\n",
      "Epoch 322/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4949 - accuracy: 0.7552 - val_loss: 0.9205 - val_accuracy: 0.5013\n",
      "Epoch 323/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4993 - accuracy: 0.7548 - val_loss: 0.9151 - val_accuracy: 0.5006\n",
      "Epoch 324/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4980 - accuracy: 0.7588 - val_loss: 0.9240 - val_accuracy: 0.4900\n",
      "Epoch 325/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5046 - accuracy: 0.7514 - val_loss: 0.9107 - val_accuracy: 0.4988\n",
      "Epoch 326/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4887 - accuracy: 0.7625 - val_loss: 0.9174 - val_accuracy: 0.4969\n",
      "Epoch 327/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4994 - accuracy: 0.7498 - val_loss: 0.9141 - val_accuracy: 0.4994\n",
      "Epoch 328/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4950 - accuracy: 0.7553 - val_loss: 0.9104 - val_accuracy: 0.5000\n",
      "Epoch 329/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5015 - accuracy: 0.7523 - val_loss: 0.9144 - val_accuracy: 0.4931\n",
      "Epoch 330/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4935 - accuracy: 0.7611 - val_loss: 0.9138 - val_accuracy: 0.5000\n",
      "Epoch 331/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5061 - accuracy: 0.7477 - val_loss: 0.9159 - val_accuracy: 0.5031\n",
      "Epoch 332/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4876 - accuracy: 0.7613 - val_loss: 0.9271 - val_accuracy: 0.5013\n",
      "Epoch 333/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5066 - accuracy: 0.7461 - val_loss: 0.9171 - val_accuracy: 0.4925\n",
      "Epoch 334/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4944 - accuracy: 0.7561 - val_loss: 0.9174 - val_accuracy: 0.4919\n",
      "Epoch 335/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4963 - accuracy: 0.7569 - val_loss: 0.9315 - val_accuracy: 0.4969\n",
      "Epoch 336/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4988 - accuracy: 0.7558 - val_loss: 0.9361 - val_accuracy: 0.4963\n",
      "Epoch 337/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4971 - accuracy: 0.7572 - val_loss: 0.9229 - val_accuracy: 0.5019\n",
      "Epoch 338/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4964 - accuracy: 0.7573 - val_loss: 0.9306 - val_accuracy: 0.5019\n",
      "Epoch 339/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5013 - accuracy: 0.7500 - val_loss: 0.9313 - val_accuracy: 0.4944\n",
      "Epoch 340/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5040 - accuracy: 0.7478 - val_loss: 0.9227 - val_accuracy: 0.5025\n",
      "Epoch 341/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5015 - accuracy: 0.7480 - val_loss: 0.9262 - val_accuracy: 0.4931\n",
      "Epoch 342/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5055 - accuracy: 0.7491 - val_loss: 0.9282 - val_accuracy: 0.5006\n",
      "Epoch 343/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4961 - accuracy: 0.7581 - val_loss: 0.9265 - val_accuracy: 0.5000\n",
      "Epoch 344/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4991 - accuracy: 0.7517 - val_loss: 0.9231 - val_accuracy: 0.4981\n",
      "Epoch 345/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4940 - accuracy: 0.7584 - val_loss: 0.9298 - val_accuracy: 0.4900\n",
      "Epoch 346/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4941 - accuracy: 0.7586 - val_loss: 0.9271 - val_accuracy: 0.4888\n",
      "Epoch 347/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5049 - accuracy: 0.7514 - val_loss: 0.9159 - val_accuracy: 0.5031\n",
      "Epoch 348/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5005 - accuracy: 0.7522 - val_loss: 0.9261 - val_accuracy: 0.4875\n",
      "Epoch 349/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4987 - accuracy: 0.7503 - val_loss: 0.9265 - val_accuracy: 0.4913\n",
      "Epoch 350/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4910 - accuracy: 0.7602 - val_loss: 0.9204 - val_accuracy: 0.4938\n",
      "Epoch 351/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4927 - accuracy: 0.7547 - val_loss: 0.9248 - val_accuracy: 0.5006\n",
      "Epoch 352/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4877 - accuracy: 0.7613 - val_loss: 0.9273 - val_accuracy: 0.4906\n",
      "Epoch 353/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4902 - accuracy: 0.7628 - val_loss: 0.9209 - val_accuracy: 0.5044\n",
      "Epoch 354/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4854 - accuracy: 0.7623 - val_loss: 0.9288 - val_accuracy: 0.4994\n",
      "Epoch 355/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4986 - accuracy: 0.7544 - val_loss: 0.9346 - val_accuracy: 0.5019\n",
      "Epoch 356/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4885 - accuracy: 0.7603 - val_loss: 0.9352 - val_accuracy: 0.5006\n",
      "Epoch 357/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4929 - accuracy: 0.7633 - val_loss: 0.9395 - val_accuracy: 0.4969\n",
      "Epoch 358/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4959 - accuracy: 0.7487 - val_loss: 0.9443 - val_accuracy: 0.5006\n",
      "Epoch 359/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5012 - accuracy: 0.7502 - val_loss: 0.9439 - val_accuracy: 0.4938\n",
      "Epoch 360/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4918 - accuracy: 0.7597 - val_loss: 0.9356 - val_accuracy: 0.4938\n",
      "Epoch 361/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4975 - accuracy: 0.7538 - val_loss: 0.9350 - val_accuracy: 0.4931\n",
      "Epoch 362/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4836 - accuracy: 0.7581 - val_loss: 0.9386 - val_accuracy: 0.4975\n",
      "Epoch 363/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4900 - accuracy: 0.7495 - val_loss: 0.9552 - val_accuracy: 0.4900\n",
      "Epoch 364/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4888 - accuracy: 0.7659 - val_loss: 0.9433 - val_accuracy: 0.4950\n",
      "Epoch 365/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4916 - accuracy: 0.7566 - val_loss: 0.9346 - val_accuracy: 0.5106\n",
      "Epoch 366/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4847 - accuracy: 0.7655 - val_loss: 0.9423 - val_accuracy: 0.4931\n",
      "Epoch 367/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4888 - accuracy: 0.7609 - val_loss: 0.9437 - val_accuracy: 0.4956\n",
      "Epoch 368/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4991 - accuracy: 0.7570 - val_loss: 0.9451 - val_accuracy: 0.4875\n",
      "Epoch 369/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4923 - accuracy: 0.7586 - val_loss: 0.9515 - val_accuracy: 0.4825\n",
      "Epoch 370/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4889 - accuracy: 0.7613 - val_loss: 0.9466 - val_accuracy: 0.4856\n",
      "Epoch 371/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4826 - accuracy: 0.7666 - val_loss: 0.9442 - val_accuracy: 0.4931\n",
      "Epoch 372/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4831 - accuracy: 0.7595 - val_loss: 0.9479 - val_accuracy: 0.4856\n",
      "Epoch 373/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4957 - accuracy: 0.7508 - val_loss: 0.9467 - val_accuracy: 0.4938\n",
      "Epoch 374/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4860 - accuracy: 0.7594 - val_loss: 0.9495 - val_accuracy: 0.4981\n",
      "Epoch 375/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4909 - accuracy: 0.7631 - val_loss: 0.9416 - val_accuracy: 0.4950\n",
      "Epoch 376/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4932 - accuracy: 0.7631 - val_loss: 0.9490 - val_accuracy: 0.4894\n",
      "Epoch 377/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4948 - accuracy: 0.7548 - val_loss: 0.9517 - val_accuracy: 0.4981\n",
      "Epoch 378/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4851 - accuracy: 0.7666 - val_loss: 0.9445 - val_accuracy: 0.5031\n",
      "Epoch 379/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4894 - accuracy: 0.7584 - val_loss: 0.9400 - val_accuracy: 0.5000\n",
      "Epoch 380/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4866 - accuracy: 0.7627 - val_loss: 0.9640 - val_accuracy: 0.4913\n",
      "Epoch 381/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5019 - accuracy: 0.7506 - val_loss: 0.9455 - val_accuracy: 0.4963\n",
      "Epoch 382/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4872 - accuracy: 0.7616 - val_loss: 0.9376 - val_accuracy: 0.4963\n",
      "Epoch 383/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4917 - accuracy: 0.7580 - val_loss: 0.9513 - val_accuracy: 0.4894\n",
      "Epoch 384/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4921 - accuracy: 0.7608 - val_loss: 0.9583 - val_accuracy: 0.4913\n",
      "Epoch 385/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4858 - accuracy: 0.7589 - val_loss: 0.9566 - val_accuracy: 0.4894\n",
      "Epoch 386/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4866 - accuracy: 0.7620 - val_loss: 0.9417 - val_accuracy: 0.5044\n",
      "Epoch 387/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4885 - accuracy: 0.7597 - val_loss: 0.9544 - val_accuracy: 0.4988\n",
      "Epoch 388/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4850 - accuracy: 0.7652 - val_loss: 0.9535 - val_accuracy: 0.4963\n",
      "Epoch 389/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4836 - accuracy: 0.7652 - val_loss: 0.9529 - val_accuracy: 0.4894\n",
      "Epoch 390/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4865 - accuracy: 0.7606 - val_loss: 0.9641 - val_accuracy: 0.4869\n",
      "Epoch 391/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4894 - accuracy: 0.7609 - val_loss: 0.9595 - val_accuracy: 0.4850\n",
      "Epoch 392/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4825 - accuracy: 0.7633 - val_loss: 0.9495 - val_accuracy: 0.4956\n",
      "Epoch 393/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4900 - accuracy: 0.7650 - val_loss: 0.9412 - val_accuracy: 0.4981\n",
      "Epoch 394/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4849 - accuracy: 0.7577 - val_loss: 0.9481 - val_accuracy: 0.4931\n",
      "Epoch 395/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4860 - accuracy: 0.7644 - val_loss: 0.9469 - val_accuracy: 0.5019\n",
      "Epoch 396/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4942 - accuracy: 0.7603 - val_loss: 0.9403 - val_accuracy: 0.4969\n",
      "Epoch 397/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5052 - accuracy: 0.7491 - val_loss: 0.9429 - val_accuracy: 0.4981\n",
      "Epoch 398/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4841 - accuracy: 0.7592 - val_loss: 0.9449 - val_accuracy: 0.4925\n",
      "Epoch 399/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4986 - accuracy: 0.7553 - val_loss: 0.9475 - val_accuracy: 0.4931\n",
      "Epoch 400/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4888 - accuracy: 0.7602 - val_loss: 0.9463 - val_accuracy: 0.4931\n",
      "Epoch 401/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4831 - accuracy: 0.7645 - val_loss: 0.9504 - val_accuracy: 0.4944\n",
      "Epoch 402/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4816 - accuracy: 0.7636 - val_loss: 0.9545 - val_accuracy: 0.5044\n",
      "Epoch 403/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4837 - accuracy: 0.7628 - val_loss: 0.9515 - val_accuracy: 0.4994\n",
      "Epoch 404/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4900 - accuracy: 0.7620 - val_loss: 0.9615 - val_accuracy: 0.4869\n",
      "Epoch 405/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4896 - accuracy: 0.7548 - val_loss: 0.9550 - val_accuracy: 0.4894\n",
      "Epoch 406/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4906 - accuracy: 0.7584 - val_loss: 0.9581 - val_accuracy: 0.4844\n",
      "Epoch 407/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4797 - accuracy: 0.7705 - val_loss: 0.9506 - val_accuracy: 0.4956\n",
      "Epoch 408/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4808 - accuracy: 0.7641 - val_loss: 0.9595 - val_accuracy: 0.4975\n",
      "Epoch 409/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4823 - accuracy: 0.7622 - val_loss: 0.9472 - val_accuracy: 0.4981\n",
      "Epoch 410/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4889 - accuracy: 0.7636 - val_loss: 0.9449 - val_accuracy: 0.5006\n",
      "Epoch 411/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4809 - accuracy: 0.7620 - val_loss: 0.9478 - val_accuracy: 0.4938\n",
      "Epoch 412/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4898 - accuracy: 0.7597 - val_loss: 0.9484 - val_accuracy: 0.4944\n",
      "Epoch 413/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4862 - accuracy: 0.7586 - val_loss: 0.9660 - val_accuracy: 0.4975\n",
      "Epoch 414/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4848 - accuracy: 0.7619 - val_loss: 0.9617 - val_accuracy: 0.5019\n",
      "Epoch 415/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4978 - accuracy: 0.7538 - val_loss: 0.9563 - val_accuracy: 0.4931\n",
      "Epoch 416/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4824 - accuracy: 0.7602 - val_loss: 0.9455 - val_accuracy: 0.5031\n",
      "Epoch 417/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4652 - accuracy: 0.7789 - val_loss: 0.9570 - val_accuracy: 0.4906\n",
      "Epoch 418/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4866 - accuracy: 0.7650 - val_loss: 0.9720 - val_accuracy: 0.4888\n",
      "Epoch 419/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4819 - accuracy: 0.7667 - val_loss: 0.9588 - val_accuracy: 0.4950\n",
      "Epoch 420/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4947 - accuracy: 0.7588 - val_loss: 0.9560 - val_accuracy: 0.4925\n",
      "Epoch 421/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4738 - accuracy: 0.7692 - val_loss: 0.9683 - val_accuracy: 0.5000\n",
      "Epoch 422/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4877 - accuracy: 0.7564 - val_loss: 0.9525 - val_accuracy: 0.5000\n",
      "Epoch 423/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4844 - accuracy: 0.7614 - val_loss: 0.9648 - val_accuracy: 0.4938\n",
      "Epoch 424/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4851 - accuracy: 0.7673 - val_loss: 0.9570 - val_accuracy: 0.4931\n",
      "Epoch 425/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4868 - accuracy: 0.7611 - val_loss: 0.9635 - val_accuracy: 0.4913\n",
      "Epoch 426/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4873 - accuracy: 0.7622 - val_loss: 0.9528 - val_accuracy: 0.5031\n",
      "Epoch 427/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4795 - accuracy: 0.7705 - val_loss: 0.9508 - val_accuracy: 0.4900\n",
      "Epoch 428/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4896 - accuracy: 0.7584 - val_loss: 0.9521 - val_accuracy: 0.4950\n",
      "Epoch 429/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4859 - accuracy: 0.7641 - val_loss: 0.9564 - val_accuracy: 0.4975\n",
      "Epoch 430/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4767 - accuracy: 0.7669 - val_loss: 0.9610 - val_accuracy: 0.4919\n",
      "Epoch 431/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4821 - accuracy: 0.7675 - val_loss: 0.9637 - val_accuracy: 0.5006\n",
      "Epoch 432/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4794 - accuracy: 0.7627 - val_loss: 0.9578 - val_accuracy: 0.4919\n",
      "Epoch 433/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4782 - accuracy: 0.7703 - val_loss: 0.9606 - val_accuracy: 0.4950\n",
      "Epoch 434/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4858 - accuracy: 0.7614 - val_loss: 0.9598 - val_accuracy: 0.4969\n",
      "Epoch 435/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4734 - accuracy: 0.7748 - val_loss: 0.9596 - val_accuracy: 0.5013\n",
      "Epoch 436/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4890 - accuracy: 0.7614 - val_loss: 0.9573 - val_accuracy: 0.5000\n",
      "Epoch 437/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4762 - accuracy: 0.7730 - val_loss: 0.9598 - val_accuracy: 0.5056\n",
      "Epoch 438/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4865 - accuracy: 0.7686 - val_loss: 0.9564 - val_accuracy: 0.4981\n",
      "Epoch 439/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4949 - accuracy: 0.7559 - val_loss: 0.9504 - val_accuracy: 0.4925\n",
      "Epoch 440/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4837 - accuracy: 0.7666 - val_loss: 0.9648 - val_accuracy: 0.4931\n",
      "Epoch 441/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4803 - accuracy: 0.7663 - val_loss: 0.9664 - val_accuracy: 0.4988\n",
      "Epoch 442/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4854 - accuracy: 0.7613 - val_loss: 0.9617 - val_accuracy: 0.4906\n",
      "Epoch 443/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4820 - accuracy: 0.7609 - val_loss: 0.9535 - val_accuracy: 0.4888\n",
      "Epoch 444/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4768 - accuracy: 0.7686 - val_loss: 0.9612 - val_accuracy: 0.4931\n",
      "Epoch 445/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4696 - accuracy: 0.7708 - val_loss: 0.9675 - val_accuracy: 0.4950\n",
      "Epoch 446/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4765 - accuracy: 0.7663 - val_loss: 0.9598 - val_accuracy: 0.4938\n",
      "Epoch 447/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4797 - accuracy: 0.7613 - val_loss: 0.9652 - val_accuracy: 0.4894\n",
      "Epoch 448/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4794 - accuracy: 0.7644 - val_loss: 0.9638 - val_accuracy: 0.4925\n",
      "Epoch 449/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4881 - accuracy: 0.7614 - val_loss: 0.9616 - val_accuracy: 0.4925\n",
      "Epoch 450/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4818 - accuracy: 0.7661 - val_loss: 0.9655 - val_accuracy: 0.4819\n",
      "Epoch 451/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4928 - accuracy: 0.7650 - val_loss: 0.9639 - val_accuracy: 0.5006\n",
      "Epoch 452/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4807 - accuracy: 0.7602 - val_loss: 0.9545 - val_accuracy: 0.4944\n",
      "Epoch 453/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4795 - accuracy: 0.7727 - val_loss: 0.9623 - val_accuracy: 0.4938\n",
      "Epoch 454/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4764 - accuracy: 0.7634 - val_loss: 0.9555 - val_accuracy: 0.4869\n",
      "Epoch 455/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4774 - accuracy: 0.7722 - val_loss: 0.9530 - val_accuracy: 0.4938\n",
      "Epoch 456/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4764 - accuracy: 0.7663 - val_loss: 0.9544 - val_accuracy: 0.4938\n",
      "Epoch 457/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4870 - accuracy: 0.7633 - val_loss: 0.9650 - val_accuracy: 0.5094\n",
      "Epoch 458/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4820 - accuracy: 0.7713 - val_loss: 0.9689 - val_accuracy: 0.4869\n",
      "Epoch 459/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4711 - accuracy: 0.7769 - val_loss: 0.9679 - val_accuracy: 0.4950\n",
      "Epoch 460/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4730 - accuracy: 0.7711 - val_loss: 0.9722 - val_accuracy: 0.4900\n",
      "Epoch 461/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4754 - accuracy: 0.7647 - val_loss: 0.9715 - val_accuracy: 0.4819\n",
      "Epoch 462/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4805 - accuracy: 0.7698 - val_loss: 0.9682 - val_accuracy: 0.4925\n",
      "Epoch 463/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4811 - accuracy: 0.7583 - val_loss: 0.9577 - val_accuracy: 0.4875\n",
      "Epoch 464/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4813 - accuracy: 0.7636 - val_loss: 0.9801 - val_accuracy: 0.4837\n",
      "Epoch 465/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4748 - accuracy: 0.7655 - val_loss: 0.9762 - val_accuracy: 0.4950\n",
      "Epoch 466/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4810 - accuracy: 0.7622 - val_loss: 0.9665 - val_accuracy: 0.4881\n",
      "Epoch 467/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4849 - accuracy: 0.7625 - val_loss: 0.9702 - val_accuracy: 0.4831\n",
      "Epoch 468/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4711 - accuracy: 0.7752 - val_loss: 0.9699 - val_accuracy: 0.4837\n",
      "Epoch 469/1000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4732 - accuracy: 0.7698 - val_loss: 0.9737 - val_accuracy: 0.4837\n",
      "Epoch 470/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4803 - accuracy: 0.7659 - val_loss: 0.9731 - val_accuracy: 0.4800\n",
      "Epoch 471/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4841 - accuracy: 0.7666 - val_loss: 0.9680 - val_accuracy: 0.4913\n",
      "Epoch 472/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4824 - accuracy: 0.7742 - val_loss: 0.9696 - val_accuracy: 0.4806\n",
      "Epoch 473/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4740 - accuracy: 0.7714 - val_loss: 0.9726 - val_accuracy: 0.4963\n",
      "Epoch 474/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4849 - accuracy: 0.7667 - val_loss: 0.9702 - val_accuracy: 0.4900\n",
      "Epoch 475/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4755 - accuracy: 0.7736 - val_loss: 0.9760 - val_accuracy: 0.4794\n",
      "Epoch 476/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4762 - accuracy: 0.7647 - val_loss: 0.9699 - val_accuracy: 0.4875\n",
      "Epoch 477/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4733 - accuracy: 0.7705 - val_loss: 0.9727 - val_accuracy: 0.4863\n",
      "Epoch 478/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4717 - accuracy: 0.7736 - val_loss: 0.9726 - val_accuracy: 0.4863\n",
      "Epoch 479/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4736 - accuracy: 0.7742 - val_loss: 0.9688 - val_accuracy: 0.4863\n",
      "Epoch 480/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4732 - accuracy: 0.7709 - val_loss: 0.9677 - val_accuracy: 0.4850\n",
      "Epoch 481/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4778 - accuracy: 0.7672 - val_loss: 0.9739 - val_accuracy: 0.4819\n",
      "Epoch 482/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4810 - accuracy: 0.7723 - val_loss: 0.9784 - val_accuracy: 0.4819\n",
      "Epoch 483/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4753 - accuracy: 0.7731 - val_loss: 0.9626 - val_accuracy: 0.4944\n",
      "Epoch 484/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4763 - accuracy: 0.7789 - val_loss: 0.9788 - val_accuracy: 0.4944\n",
      "Epoch 485/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4752 - accuracy: 0.7661 - val_loss: 0.9686 - val_accuracy: 0.4938\n",
      "Epoch 486/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4766 - accuracy: 0.7695 - val_loss: 0.9648 - val_accuracy: 0.4925\n",
      "Epoch 487/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4778 - accuracy: 0.7653 - val_loss: 0.9693 - val_accuracy: 0.4913\n",
      "Epoch 488/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4710 - accuracy: 0.7733 - val_loss: 0.9699 - val_accuracy: 0.4925\n",
      "Epoch 489/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4660 - accuracy: 0.7808 - val_loss: 0.9660 - val_accuracy: 0.4863\n",
      "Epoch 490/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4798 - accuracy: 0.7670 - val_loss: 0.9769 - val_accuracy: 0.4762\n",
      "Epoch 491/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4781 - accuracy: 0.7677 - val_loss: 0.9634 - val_accuracy: 0.4837\n",
      "Epoch 492/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4791 - accuracy: 0.7664 - val_loss: 0.9681 - val_accuracy: 0.4931\n",
      "Epoch 493/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4719 - accuracy: 0.7736 - val_loss: 0.9646 - val_accuracy: 0.4956\n",
      "Epoch 494/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4751 - accuracy: 0.7703 - val_loss: 0.9674 - val_accuracy: 0.4919\n",
      "Epoch 495/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4662 - accuracy: 0.7773 - val_loss: 0.9863 - val_accuracy: 0.4888\n",
      "Epoch 496/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4724 - accuracy: 0.7686 - val_loss: 0.9772 - val_accuracy: 0.4919\n",
      "Epoch 497/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4722 - accuracy: 0.7703 - val_loss: 0.9671 - val_accuracy: 0.4981\n",
      "Epoch 498/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4730 - accuracy: 0.7663 - val_loss: 0.9720 - val_accuracy: 0.4894\n",
      "Epoch 499/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4963 - accuracy: 0.7566 - val_loss: 0.9659 - val_accuracy: 0.5006\n",
      "Epoch 500/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4705 - accuracy: 0.7728 - val_loss: 0.9738 - val_accuracy: 0.4969\n",
      "Epoch 501/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4717 - accuracy: 0.7697 - val_loss: 0.9726 - val_accuracy: 0.5063\n",
      "Epoch 502/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4783 - accuracy: 0.7645 - val_loss: 0.9719 - val_accuracy: 0.5056\n",
      "Epoch 503/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4776 - accuracy: 0.7667 - val_loss: 0.9700 - val_accuracy: 0.5000\n",
      "Epoch 504/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4783 - accuracy: 0.7709 - val_loss: 0.9656 - val_accuracy: 0.5031\n",
      "Epoch 505/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4725 - accuracy: 0.7708 - val_loss: 0.9651 - val_accuracy: 0.5069\n",
      "Epoch 506/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4729 - accuracy: 0.7698 - val_loss: 0.9595 - val_accuracy: 0.4994\n",
      "Epoch 507/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4827 - accuracy: 0.7658 - val_loss: 0.9625 - val_accuracy: 0.4881\n",
      "Epoch 508/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4743 - accuracy: 0.7695 - val_loss: 0.9564 - val_accuracy: 0.5063\n",
      "Epoch 509/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4640 - accuracy: 0.7755 - val_loss: 0.9792 - val_accuracy: 0.4888\n",
      "Epoch 510/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4690 - accuracy: 0.7839 - val_loss: 0.9618 - val_accuracy: 0.4975\n",
      "Epoch 511/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4811 - accuracy: 0.7648 - val_loss: 0.9618 - val_accuracy: 0.5013\n",
      "Epoch 512/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4733 - accuracy: 0.7695 - val_loss: 0.9677 - val_accuracy: 0.4956\n",
      "Epoch 513/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4641 - accuracy: 0.7738 - val_loss: 0.9758 - val_accuracy: 0.4869\n",
      "Epoch 514/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4836 - accuracy: 0.7658 - val_loss: 0.9923 - val_accuracy: 0.4781\n",
      "Epoch 515/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4683 - accuracy: 0.7773 - val_loss: 0.9736 - val_accuracy: 0.4888\n",
      "Epoch 516/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4765 - accuracy: 0.7741 - val_loss: 0.9837 - val_accuracy: 0.4881\n",
      "Epoch 517/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4783 - accuracy: 0.7697 - val_loss: 0.9743 - val_accuracy: 0.4881\n",
      "Epoch 518/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4645 - accuracy: 0.7725 - val_loss: 0.9823 - val_accuracy: 0.4856\n",
      "Epoch 519/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4764 - accuracy: 0.7734 - val_loss: 0.9742 - val_accuracy: 0.4769\n",
      "Epoch 520/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4729 - accuracy: 0.7731 - val_loss: 0.9825 - val_accuracy: 0.4800\n",
      "Epoch 521/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4642 - accuracy: 0.7817 - val_loss: 0.9853 - val_accuracy: 0.4819\n",
      "Epoch 522/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4835 - accuracy: 0.7656 - val_loss: 0.9750 - val_accuracy: 0.4975\n",
      "Epoch 523/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4759 - accuracy: 0.7714 - val_loss: 0.9855 - val_accuracy: 0.4881\n",
      "Epoch 524/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4702 - accuracy: 0.7747 - val_loss: 0.9853 - val_accuracy: 0.4888\n",
      "Epoch 525/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4715 - accuracy: 0.7725 - val_loss: 0.9785 - val_accuracy: 0.4931\n",
      "Epoch 526/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4722 - accuracy: 0.7686 - val_loss: 0.9757 - val_accuracy: 0.4869\n",
      "Epoch 527/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4761 - accuracy: 0.7742 - val_loss: 0.9760 - val_accuracy: 0.4938\n",
      "Epoch 528/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4676 - accuracy: 0.7709 - val_loss: 0.9718 - val_accuracy: 0.4950\n",
      "Epoch 529/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4601 - accuracy: 0.7817 - val_loss: 0.9844 - val_accuracy: 0.4944\n",
      "Epoch 530/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4607 - accuracy: 0.7784 - val_loss: 0.9796 - val_accuracy: 0.4963\n",
      "Epoch 531/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4732 - accuracy: 0.7708 - val_loss: 0.9882 - val_accuracy: 0.4850\n",
      "Epoch 532/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4698 - accuracy: 0.7722 - val_loss: 0.9693 - val_accuracy: 0.4931\n",
      "Epoch 533/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4773 - accuracy: 0.7680 - val_loss: 0.9680 - val_accuracy: 0.4944\n",
      "Epoch 534/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4677 - accuracy: 0.7786 - val_loss: 0.9704 - val_accuracy: 0.4956\n",
      "Epoch 535/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4658 - accuracy: 0.7778 - val_loss: 0.9850 - val_accuracy: 0.4881\n",
      "Epoch 536/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4787 - accuracy: 0.7613 - val_loss: 0.9792 - val_accuracy: 0.4869\n",
      "Epoch 537/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4823 - accuracy: 0.7642 - val_loss: 0.9726 - val_accuracy: 0.4881\n",
      "Epoch 538/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4679 - accuracy: 0.7695 - val_loss: 0.9705 - val_accuracy: 0.4981\n",
      "Epoch 539/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4668 - accuracy: 0.7781 - val_loss: 0.9649 - val_accuracy: 0.5000\n",
      "Epoch 540/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4633 - accuracy: 0.7756 - val_loss: 0.9757 - val_accuracy: 0.4888\n",
      "Epoch 541/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4643 - accuracy: 0.7739 - val_loss: 0.9729 - val_accuracy: 0.4938\n",
      "Epoch 542/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4677 - accuracy: 0.7747 - val_loss: 0.9683 - val_accuracy: 0.4975\n",
      "Epoch 543/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4732 - accuracy: 0.7717 - val_loss: 0.9704 - val_accuracy: 0.4944\n",
      "Epoch 544/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4737 - accuracy: 0.7753 - val_loss: 0.9762 - val_accuracy: 0.4875\n",
      "Epoch 545/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4718 - accuracy: 0.7734 - val_loss: 0.9808 - val_accuracy: 0.4963\n",
      "Epoch 546/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4696 - accuracy: 0.7753 - val_loss: 0.9737 - val_accuracy: 0.5038\n",
      "Epoch 547/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4619 - accuracy: 0.7770 - val_loss: 0.9825 - val_accuracy: 0.4931\n",
      "Epoch 548/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4767 - accuracy: 0.7650 - val_loss: 0.9639 - val_accuracy: 0.5050\n",
      "Epoch 549/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4735 - accuracy: 0.7755 - val_loss: 0.9774 - val_accuracy: 0.4938\n",
      "Epoch 550/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4655 - accuracy: 0.7792 - val_loss: 0.9761 - val_accuracy: 0.4906\n",
      "Epoch 551/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4659 - accuracy: 0.7750 - val_loss: 1.0049 - val_accuracy: 0.4919\n",
      "Epoch 552/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4718 - accuracy: 0.7717 - val_loss: 0.9850 - val_accuracy: 0.4906\n",
      "Epoch 553/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4745 - accuracy: 0.7711 - val_loss: 0.9877 - val_accuracy: 0.4963\n",
      "Epoch 554/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4591 - accuracy: 0.7834 - val_loss: 0.9897 - val_accuracy: 0.4888\n",
      "Epoch 555/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4718 - accuracy: 0.7722 - val_loss: 0.9849 - val_accuracy: 0.4863\n",
      "Epoch 556/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4646 - accuracy: 0.7814 - val_loss: 0.9780 - val_accuracy: 0.4881\n",
      "Epoch 557/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4634 - accuracy: 0.7775 - val_loss: 0.9934 - val_accuracy: 0.4931\n",
      "Epoch 558/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4692 - accuracy: 0.7750 - val_loss: 0.9917 - val_accuracy: 0.4856\n",
      "Epoch 559/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4664 - accuracy: 0.7770 - val_loss: 0.9878 - val_accuracy: 0.4888\n",
      "Epoch 560/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4588 - accuracy: 0.7822 - val_loss: 0.9847 - val_accuracy: 0.4931\n",
      "Epoch 561/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4689 - accuracy: 0.7791 - val_loss: 0.9833 - val_accuracy: 0.4869\n",
      "Epoch 562/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4665 - accuracy: 0.7770 - val_loss: 0.9935 - val_accuracy: 0.4863\n",
      "Epoch 563/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4622 - accuracy: 0.7800 - val_loss: 0.9899 - val_accuracy: 0.4837\n",
      "Epoch 564/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4627 - accuracy: 0.7812 - val_loss: 0.9907 - val_accuracy: 0.4794\n",
      "Epoch 565/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4599 - accuracy: 0.7831 - val_loss: 0.9958 - val_accuracy: 0.4837\n",
      "Epoch 566/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4593 - accuracy: 0.7823 - val_loss: 0.9879 - val_accuracy: 0.4831\n",
      "Epoch 567/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4652 - accuracy: 0.7741 - val_loss: 0.9911 - val_accuracy: 0.4863\n",
      "Epoch 568/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4696 - accuracy: 0.7773 - val_loss: 0.9963 - val_accuracy: 0.4800\n",
      "Epoch 569/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4653 - accuracy: 0.7825 - val_loss: 0.9915 - val_accuracy: 0.4875\n",
      "Epoch 570/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4566 - accuracy: 0.7814 - val_loss: 0.9936 - val_accuracy: 0.4881\n",
      "Epoch 571/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4638 - accuracy: 0.7763 - val_loss: 0.9950 - val_accuracy: 0.4981\n",
      "Epoch 572/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4704 - accuracy: 0.7761 - val_loss: 0.9942 - val_accuracy: 0.4925\n",
      "Epoch 573/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4622 - accuracy: 0.7812 - val_loss: 0.9812 - val_accuracy: 0.4975\n",
      "Epoch 574/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4671 - accuracy: 0.7816 - val_loss: 0.9732 - val_accuracy: 0.5050\n",
      "Epoch 575/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4526 - accuracy: 0.7884 - val_loss: 0.9841 - val_accuracy: 0.4988\n",
      "Epoch 576/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4598 - accuracy: 0.7770 - val_loss: 0.9855 - val_accuracy: 0.4913\n",
      "Epoch 577/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4720 - accuracy: 0.7714 - val_loss: 0.9809 - val_accuracy: 0.4975\n",
      "Epoch 578/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4674 - accuracy: 0.7809 - val_loss: 0.9860 - val_accuracy: 0.4963\n",
      "Epoch 579/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4641 - accuracy: 0.7788 - val_loss: 0.9788 - val_accuracy: 0.4913\n",
      "Epoch 580/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4589 - accuracy: 0.7812 - val_loss: 0.9862 - val_accuracy: 0.4913\n",
      "Epoch 581/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4642 - accuracy: 0.7844 - val_loss: 0.9766 - val_accuracy: 0.4938\n",
      "Epoch 582/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4659 - accuracy: 0.7808 - val_loss: 0.9750 - val_accuracy: 0.4850\n",
      "Epoch 583/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4530 - accuracy: 0.7816 - val_loss: 0.9832 - val_accuracy: 0.4881\n",
      "Epoch 584/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4646 - accuracy: 0.7775 - val_loss: 0.9925 - val_accuracy: 0.4900\n",
      "Epoch 585/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4608 - accuracy: 0.7795 - val_loss: 0.9908 - val_accuracy: 0.4863\n",
      "Epoch 586/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4631 - accuracy: 0.7797 - val_loss: 1.0016 - val_accuracy: 0.4844\n",
      "Epoch 587/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4668 - accuracy: 0.7756 - val_loss: 0.9991 - val_accuracy: 0.4863\n",
      "Epoch 588/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4553 - accuracy: 0.7872 - val_loss: 0.9960 - val_accuracy: 0.4944\n",
      "Epoch 589/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4674 - accuracy: 0.7788 - val_loss: 0.9979 - val_accuracy: 0.4856\n",
      "Epoch 590/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4765 - accuracy: 0.7713 - val_loss: 0.9925 - val_accuracy: 0.4950\n",
      "Epoch 591/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4681 - accuracy: 0.7773 - val_loss: 0.9831 - val_accuracy: 0.4925\n",
      "Epoch 592/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4613 - accuracy: 0.7828 - val_loss: 0.9876 - val_accuracy: 0.5006\n",
      "Epoch 593/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4621 - accuracy: 0.7791 - val_loss: 0.9882 - val_accuracy: 0.5081\n",
      "Epoch 594/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4701 - accuracy: 0.7744 - val_loss: 0.9874 - val_accuracy: 0.5050\n",
      "Epoch 595/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4604 - accuracy: 0.7805 - val_loss: 0.9851 - val_accuracy: 0.4919\n",
      "Epoch 596/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4631 - accuracy: 0.7781 - val_loss: 0.9856 - val_accuracy: 0.4925\n",
      "Epoch 597/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4669 - accuracy: 0.7783 - val_loss: 0.9844 - val_accuracy: 0.4944\n",
      "Epoch 598/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4673 - accuracy: 0.7769 - val_loss: 0.9858 - val_accuracy: 0.4981\n",
      "Epoch 599/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4664 - accuracy: 0.7744 - val_loss: 0.9890 - val_accuracy: 0.4950\n",
      "Epoch 600/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4735 - accuracy: 0.7689 - val_loss: 0.9829 - val_accuracy: 0.4906\n",
      "Epoch 601/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4638 - accuracy: 0.7741 - val_loss: 0.9869 - val_accuracy: 0.4950\n",
      "Epoch 602/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4678 - accuracy: 0.7744 - val_loss: 0.9921 - val_accuracy: 0.4906\n",
      "Epoch 603/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4670 - accuracy: 0.7719 - val_loss: 0.9871 - val_accuracy: 0.5056\n",
      "Epoch 604/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4618 - accuracy: 0.7825 - val_loss: 0.9805 - val_accuracy: 0.4969\n",
      "Epoch 605/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4551 - accuracy: 0.7850 - val_loss: 0.9800 - val_accuracy: 0.4938\n",
      "Epoch 606/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4743 - accuracy: 0.7697 - val_loss: 0.9833 - val_accuracy: 0.4950\n",
      "Epoch 607/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4692 - accuracy: 0.7789 - val_loss: 0.9865 - val_accuracy: 0.4969\n",
      "Epoch 608/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4605 - accuracy: 0.7748 - val_loss: 0.9829 - val_accuracy: 0.4794\n",
      "Epoch 609/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4700 - accuracy: 0.7745 - val_loss: 0.9817 - val_accuracy: 0.4875\n",
      "Epoch 610/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4655 - accuracy: 0.7773 - val_loss: 0.9823 - val_accuracy: 0.4925\n",
      "Epoch 611/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4691 - accuracy: 0.7742 - val_loss: 0.9787 - val_accuracy: 0.4869\n",
      "Epoch 612/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4697 - accuracy: 0.7723 - val_loss: 0.9895 - val_accuracy: 0.4856\n",
      "Epoch 613/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4592 - accuracy: 0.7805 - val_loss: 0.9729 - val_accuracy: 0.4938\n",
      "Epoch 614/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4635 - accuracy: 0.7817 - val_loss: 0.9827 - val_accuracy: 0.4888\n",
      "Epoch 615/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4625 - accuracy: 0.7816 - val_loss: 0.9832 - val_accuracy: 0.4894\n",
      "Epoch 616/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4596 - accuracy: 0.7773 - val_loss: 0.9934 - val_accuracy: 0.4875\n",
      "Epoch 617/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4648 - accuracy: 0.7834 - val_loss: 0.9994 - val_accuracy: 0.4869\n",
      "Epoch 618/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4585 - accuracy: 0.7808 - val_loss: 0.9919 - val_accuracy: 0.4925\n",
      "Epoch 619/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4663 - accuracy: 0.7783 - val_loss: 0.9885 - val_accuracy: 0.4812\n",
      "Epoch 620/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4550 - accuracy: 0.7786 - val_loss: 0.9953 - val_accuracy: 0.4900\n",
      "Epoch 621/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4553 - accuracy: 0.7833 - val_loss: 0.9947 - val_accuracy: 0.4844\n",
      "Epoch 622/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4533 - accuracy: 0.7864 - val_loss: 0.9914 - val_accuracy: 0.4956\n",
      "Epoch 623/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4539 - accuracy: 0.7822 - val_loss: 0.9993 - val_accuracy: 0.4831\n",
      "Epoch 624/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4637 - accuracy: 0.7845 - val_loss: 1.0038 - val_accuracy: 0.4812\n",
      "Epoch 625/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4587 - accuracy: 0.7763 - val_loss: 0.9955 - val_accuracy: 0.4888\n",
      "Epoch 626/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4719 - accuracy: 0.7784 - val_loss: 1.0000 - val_accuracy: 0.4906\n",
      "Epoch 627/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4615 - accuracy: 0.7814 - val_loss: 0.9937 - val_accuracy: 0.4875\n",
      "Epoch 628/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4632 - accuracy: 0.7798 - val_loss: 0.9926 - val_accuracy: 0.4906\n",
      "Epoch 629/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4703 - accuracy: 0.7670 - val_loss: 0.9911 - val_accuracy: 0.5019\n",
      "Epoch 630/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4628 - accuracy: 0.7767 - val_loss: 0.9854 - val_accuracy: 0.5013\n",
      "Epoch 631/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4650 - accuracy: 0.7747 - val_loss: 0.9830 - val_accuracy: 0.5031\n",
      "Epoch 632/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4558 - accuracy: 0.7833 - val_loss: 0.9825 - val_accuracy: 0.4913\n",
      "Epoch 633/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4573 - accuracy: 0.7827 - val_loss: 0.9879 - val_accuracy: 0.4975\n",
      "Epoch 634/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4628 - accuracy: 0.7850 - val_loss: 0.9832 - val_accuracy: 0.4994\n",
      "Epoch 635/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4605 - accuracy: 0.7819 - val_loss: 0.9737 - val_accuracy: 0.5094\n",
      "Epoch 636/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4732 - accuracy: 0.7705 - val_loss: 0.9909 - val_accuracy: 0.4963\n",
      "Epoch 637/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4618 - accuracy: 0.7747 - val_loss: 0.9975 - val_accuracy: 0.5025\n",
      "Epoch 638/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4587 - accuracy: 0.7806 - val_loss: 0.9893 - val_accuracy: 0.5000\n",
      "Epoch 639/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4631 - accuracy: 0.7728 - val_loss: 0.9912 - val_accuracy: 0.5025\n",
      "Epoch 640/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4555 - accuracy: 0.7862 - val_loss: 0.9868 - val_accuracy: 0.5019\n",
      "Epoch 641/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4654 - accuracy: 0.7736 - val_loss: 0.9839 - val_accuracy: 0.5038\n",
      "Epoch 642/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4575 - accuracy: 0.7819 - val_loss: 0.9903 - val_accuracy: 0.5006\n",
      "Epoch 643/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4589 - accuracy: 0.7783 - val_loss: 0.9923 - val_accuracy: 0.5063\n",
      "Epoch 644/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4620 - accuracy: 0.7811 - val_loss: 0.9915 - val_accuracy: 0.5025\n",
      "Epoch 645/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4541 - accuracy: 0.7842 - val_loss: 0.9828 - val_accuracy: 0.5063\n",
      "Epoch 646/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4585 - accuracy: 0.7795 - val_loss: 0.9853 - val_accuracy: 0.5056\n",
      "Epoch 647/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4593 - accuracy: 0.7858 - val_loss: 0.9923 - val_accuracy: 0.4994\n",
      "Epoch 648/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4570 - accuracy: 0.7831 - val_loss: 0.9861 - val_accuracy: 0.5025\n",
      "Epoch 649/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4609 - accuracy: 0.7825 - val_loss: 0.9862 - val_accuracy: 0.5050\n",
      "Epoch 650/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4562 - accuracy: 0.7866 - val_loss: 0.9874 - val_accuracy: 0.5075\n",
      "Epoch 651/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4494 - accuracy: 0.7884 - val_loss: 0.9869 - val_accuracy: 0.5031\n",
      "Epoch 652/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4621 - accuracy: 0.7759 - val_loss: 0.9991 - val_accuracy: 0.4950\n",
      "Epoch 653/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4590 - accuracy: 0.7755 - val_loss: 0.9961 - val_accuracy: 0.5063\n",
      "Epoch 654/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4576 - accuracy: 0.7767 - val_loss: 1.0001 - val_accuracy: 0.4988\n",
      "Epoch 655/1000\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.4716 - accuracy: 0.7739 - val_loss: 0.9892 - val_accuracy: 0.4969\n",
      "Epoch 656/1000\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.4554 - accuracy: 0.7800 - val_loss: 0.9995 - val_accuracy: 0.5000\n",
      "Epoch 657/1000\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.4633 - accuracy: 0.7808 - val_loss: 0.9765 - val_accuracy: 0.5081\n",
      "Epoch 658/1000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4587 - accuracy: 0.7764 - val_loss: 0.9863 - val_accuracy: 0.5100\n",
      "Epoch 659/1000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4642 - accuracy: 0.7759 - val_loss: 0.9835 - val_accuracy: 0.5056\n",
      "Epoch 660/1000\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.4524 - accuracy: 0.7823 - val_loss: 1.0012 - val_accuracy: 0.4931\n",
      "Epoch 661/1000\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.4623 - accuracy: 0.7781 - val_loss: 0.9927 - val_accuracy: 0.4988\n",
      "Epoch 662/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4631 - accuracy: 0.7769 - val_loss: 0.9955 - val_accuracy: 0.4994\n",
      "Epoch 663/1000\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.4571 - accuracy: 0.7834 - val_loss: 0.9856 - val_accuracy: 0.5019\n",
      "Epoch 664/1000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4579 - accuracy: 0.7853 - val_loss: 0.9893 - val_accuracy: 0.5094\n",
      "Epoch 665/1000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4610 - accuracy: 0.7806 - val_loss: 0.9887 - val_accuracy: 0.5106\n",
      "Epoch 666/1000\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.4634 - accuracy: 0.7845 - val_loss: 0.9820 - val_accuracy: 0.5044\n",
      "Epoch 667/1000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4508 - accuracy: 0.7808 - val_loss: 0.9860 - val_accuracy: 0.5025\n",
      "Epoch 668/1000\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.4538 - accuracy: 0.7817 - val_loss: 0.9807 - val_accuracy: 0.4988\n",
      "Epoch 669/1000\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.4604 - accuracy: 0.7766 - val_loss: 0.9701 - val_accuracy: 0.5044\n",
      "Epoch 670/1000\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.4625 - accuracy: 0.7745 - val_loss: 0.9721 - val_accuracy: 0.5106\n",
      "Epoch 671/1000\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4526 - accuracy: 0.7809 - val_loss: 0.9786 - val_accuracy: 0.5081\n",
      "Epoch 672/1000\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.4514 - accuracy: 0.7861 - val_loss: 0.9766 - val_accuracy: 0.5075\n",
      "Epoch 673/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4656 - accuracy: 0.7759 - val_loss: 0.9743 - val_accuracy: 0.4969\n",
      "Epoch 674/1000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4555 - accuracy: 0.7841 - val_loss: 0.9854 - val_accuracy: 0.4938\n",
      "Epoch 675/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4526 - accuracy: 0.7828 - val_loss: 0.9836 - val_accuracy: 0.4906\n",
      "Epoch 676/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4571 - accuracy: 0.7817 - val_loss: 0.9796 - val_accuracy: 0.4975\n",
      "Epoch 677/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4590 - accuracy: 0.7738 - val_loss: 0.9933 - val_accuracy: 0.4981\n",
      "Epoch 678/1000\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.4567 - accuracy: 0.7834 - val_loss: 0.9818 - val_accuracy: 0.5044\n",
      "Epoch 679/1000\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.4615 - accuracy: 0.7791 - val_loss: 0.9738 - val_accuracy: 0.5088\n",
      "Epoch 680/1000\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.4468 - accuracy: 0.7852 - val_loss: 0.9890 - val_accuracy: 0.4944\n",
      "Epoch 681/1000\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.4542 - accuracy: 0.7830 - val_loss: 0.9915 - val_accuracy: 0.4969\n",
      "Epoch 682/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4563 - accuracy: 0.7908 - val_loss: 0.9908 - val_accuracy: 0.4981\n",
      "Epoch 683/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4515 - accuracy: 0.7873 - val_loss: 0.9937 - val_accuracy: 0.4956\n",
      "Epoch 684/1000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4641 - accuracy: 0.7756 - val_loss: 0.9875 - val_accuracy: 0.4956\n",
      "Epoch 685/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4549 - accuracy: 0.7788 - val_loss: 0.9881 - val_accuracy: 0.5013\n",
      "Epoch 686/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4529 - accuracy: 0.7833 - val_loss: 0.9871 - val_accuracy: 0.4988\n",
      "Epoch 687/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4532 - accuracy: 0.7828 - val_loss: 0.9863 - val_accuracy: 0.5019\n",
      "Epoch 688/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4542 - accuracy: 0.7837 - val_loss: 0.9767 - val_accuracy: 0.5006\n",
      "Epoch 689/1000\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.4574 - accuracy: 0.7836 - val_loss: 0.9819 - val_accuracy: 0.4950\n",
      "Epoch 690/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4553 - accuracy: 0.7791 - val_loss: 0.9786 - val_accuracy: 0.4913\n",
      "Epoch 691/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4601 - accuracy: 0.7870 - val_loss: 0.9895 - val_accuracy: 0.4950\n",
      "Epoch 692/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4579 - accuracy: 0.7812 - val_loss: 0.9847 - val_accuracy: 0.4938\n",
      "Epoch 693/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4686 - accuracy: 0.7733 - val_loss: 0.9834 - val_accuracy: 0.5025\n",
      "Epoch 694/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4550 - accuracy: 0.7817 - val_loss: 0.9892 - val_accuracy: 0.4981\n",
      "Epoch 695/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4669 - accuracy: 0.7809 - val_loss: 0.9896 - val_accuracy: 0.4938\n",
      "Epoch 696/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4475 - accuracy: 0.7903 - val_loss: 0.9929 - val_accuracy: 0.4925\n",
      "Epoch 697/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4452 - accuracy: 0.7905 - val_loss: 0.9895 - val_accuracy: 0.4963\n",
      "Epoch 698/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4530 - accuracy: 0.7825 - val_loss: 0.9954 - val_accuracy: 0.4963\n",
      "Epoch 699/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4542 - accuracy: 0.7836 - val_loss: 1.0019 - val_accuracy: 0.4950\n",
      "Epoch 700/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4539 - accuracy: 0.7834 - val_loss: 0.9952 - val_accuracy: 0.4919\n",
      "Epoch 701/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4562 - accuracy: 0.7789 - val_loss: 0.9937 - val_accuracy: 0.4994\n",
      "Epoch 702/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4512 - accuracy: 0.7881 - val_loss: 0.9957 - val_accuracy: 0.4969\n",
      "Epoch 703/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4577 - accuracy: 0.7819 - val_loss: 0.9929 - val_accuracy: 0.5044\n",
      "Epoch 704/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4490 - accuracy: 0.7912 - val_loss: 0.9950 - val_accuracy: 0.4969\n",
      "Epoch 705/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4469 - accuracy: 0.7900 - val_loss: 1.0001 - val_accuracy: 0.4988\n",
      "Epoch 706/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4502 - accuracy: 0.7900 - val_loss: 0.9894 - val_accuracy: 0.5013\n",
      "Epoch 707/1000\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.4537 - accuracy: 0.7839 - val_loss: 0.9903 - val_accuracy: 0.5056\n",
      "Epoch 708/1000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4531 - accuracy: 0.7839 - val_loss: 0.9950 - val_accuracy: 0.4950\n",
      "Epoch 709/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4536 - accuracy: 0.7795 - val_loss: 0.9929 - val_accuracy: 0.4963\n",
      "Epoch 710/1000\n",
      "200/200 [==============================] - 2s 9ms/step - loss: 0.4529 - accuracy: 0.7894 - val_loss: 1.0001 - val_accuracy: 0.4981\n",
      "Epoch 711/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4558 - accuracy: 0.7880 - val_loss: 0.9917 - val_accuracy: 0.5113\n",
      "Epoch 712/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4629 - accuracy: 0.7809 - val_loss: 0.9919 - val_accuracy: 0.5069\n",
      "Epoch 713/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4566 - accuracy: 0.7819 - val_loss: 0.9896 - val_accuracy: 0.5038\n",
      "Epoch 714/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4522 - accuracy: 0.7861 - val_loss: 0.9882 - val_accuracy: 0.5013\n",
      "Epoch 715/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4488 - accuracy: 0.7909 - val_loss: 0.9898 - val_accuracy: 0.4888\n",
      "Epoch 716/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4619 - accuracy: 0.7777 - val_loss: 0.9818 - val_accuracy: 0.4988\n",
      "Epoch 717/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4573 - accuracy: 0.7920 - val_loss: 0.9779 - val_accuracy: 0.5100\n",
      "Epoch 718/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4540 - accuracy: 0.7827 - val_loss: 0.9933 - val_accuracy: 0.5044\n",
      "Epoch 719/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4573 - accuracy: 0.7833 - val_loss: 0.9923 - val_accuracy: 0.4944\n",
      "Epoch 720/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4534 - accuracy: 0.7872 - val_loss: 0.9829 - val_accuracy: 0.5038\n",
      "Epoch 721/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4407 - accuracy: 0.7919 - val_loss: 1.0035 - val_accuracy: 0.5000\n",
      "Epoch 722/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4599 - accuracy: 0.7797 - val_loss: 0.9959 - val_accuracy: 0.5019\n",
      "Epoch 723/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4457 - accuracy: 0.7886 - val_loss: 1.0013 - val_accuracy: 0.4956\n",
      "Epoch 724/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4537 - accuracy: 0.7806 - val_loss: 1.0007 - val_accuracy: 0.4969\n",
      "Epoch 725/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4552 - accuracy: 0.7881 - val_loss: 0.9927 - val_accuracy: 0.5031\n",
      "Epoch 726/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4485 - accuracy: 0.7892 - val_loss: 0.9988 - val_accuracy: 0.4988\n",
      "Epoch 727/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4627 - accuracy: 0.7797 - val_loss: 0.9973 - val_accuracy: 0.5038\n",
      "Epoch 728/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4521 - accuracy: 0.7837 - val_loss: 0.9836 - val_accuracy: 0.5113\n",
      "Epoch 729/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4654 - accuracy: 0.7748 - val_loss: 0.9820 - val_accuracy: 0.5119\n",
      "Epoch 730/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4486 - accuracy: 0.7906 - val_loss: 0.9899 - val_accuracy: 0.5019\n",
      "Epoch 731/1000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4576 - accuracy: 0.7828 - val_loss: 0.9979 - val_accuracy: 0.5038\n",
      "Epoch 732/1000\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.4489 - accuracy: 0.7894 - val_loss: 0.9995 - val_accuracy: 0.5019\n",
      "Epoch 733/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4465 - accuracy: 0.7875 - val_loss: 1.0009 - val_accuracy: 0.5006\n",
      "Epoch 734/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4465 - accuracy: 0.7942 - val_loss: 0.9913 - val_accuracy: 0.4963\n",
      "Epoch 735/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4527 - accuracy: 0.7887 - val_loss: 1.0077 - val_accuracy: 0.5025\n",
      "Epoch 736/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4598 - accuracy: 0.7764 - val_loss: 0.9956 - val_accuracy: 0.4969\n",
      "Epoch 737/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4513 - accuracy: 0.7855 - val_loss: 1.0024 - val_accuracy: 0.4931\n",
      "Epoch 738/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4536 - accuracy: 0.7864 - val_loss: 0.9976 - val_accuracy: 0.5025\n",
      "Epoch 739/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4637 - accuracy: 0.7764 - val_loss: 0.9932 - val_accuracy: 0.5050\n",
      "Epoch 740/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4577 - accuracy: 0.7816 - val_loss: 0.9887 - val_accuracy: 0.5000\n",
      "Epoch 741/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4505 - accuracy: 0.7870 - val_loss: 0.9953 - val_accuracy: 0.5031\n",
      "Epoch 742/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4513 - accuracy: 0.7852 - val_loss: 0.9926 - val_accuracy: 0.5019\n",
      "Epoch 743/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4537 - accuracy: 0.7777 - val_loss: 0.9960 - val_accuracy: 0.5025\n",
      "Epoch 744/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4590 - accuracy: 0.7822 - val_loss: 1.0015 - val_accuracy: 0.5063\n",
      "Epoch 745/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4469 - accuracy: 0.7878 - val_loss: 0.9886 - val_accuracy: 0.5119\n",
      "Epoch 746/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4601 - accuracy: 0.7789 - val_loss: 0.9898 - val_accuracy: 0.5038\n",
      "Epoch 747/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4598 - accuracy: 0.7806 - val_loss: 0.9898 - val_accuracy: 0.5025\n",
      "Epoch 748/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4496 - accuracy: 0.7853 - val_loss: 0.9871 - val_accuracy: 0.5000\n",
      "Epoch 749/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4444 - accuracy: 0.7897 - val_loss: 1.0062 - val_accuracy: 0.5006\n",
      "Epoch 750/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4450 - accuracy: 0.7895 - val_loss: 0.9970 - val_accuracy: 0.4981\n",
      "Epoch 751/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4523 - accuracy: 0.7848 - val_loss: 1.0092 - val_accuracy: 0.5025\n",
      "Epoch 752/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4399 - accuracy: 0.7905 - val_loss: 0.9995 - val_accuracy: 0.5006\n",
      "Epoch 753/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4486 - accuracy: 0.7927 - val_loss: 1.0040 - val_accuracy: 0.5038\n",
      "Epoch 754/1000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4563 - accuracy: 0.7775 - val_loss: 0.9968 - val_accuracy: 0.5063\n",
      "Epoch 755/1000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4521 - accuracy: 0.7902 - val_loss: 0.9945 - val_accuracy: 0.4988\n",
      "Epoch 756/1000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4421 - accuracy: 0.7914 - val_loss: 1.0064 - val_accuracy: 0.4925\n",
      "Epoch 757/1000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4602 - accuracy: 0.7802 - val_loss: 0.9990 - val_accuracy: 0.5006\n",
      "Epoch 758/1000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4408 - accuracy: 0.7923 - val_loss: 0.9980 - val_accuracy: 0.4988\n",
      "Epoch 759/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4506 - accuracy: 0.7898 - val_loss: 0.9977 - val_accuracy: 0.4950\n",
      "Epoch 760/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4527 - accuracy: 0.7883 - val_loss: 1.0036 - val_accuracy: 0.5000\n",
      "Epoch 761/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4599 - accuracy: 0.7825 - val_loss: 0.9916 - val_accuracy: 0.4969\n",
      "Epoch 762/1000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4491 - accuracy: 0.7925 - val_loss: 0.9999 - val_accuracy: 0.4988\n",
      "Epoch 763/1000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4449 - accuracy: 0.7934 - val_loss: 1.0069 - val_accuracy: 0.5000\n",
      "Epoch 764/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4450 - accuracy: 0.7855 - val_loss: 1.0066 - val_accuracy: 0.4988\n",
      "Epoch 765/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4659 - accuracy: 0.7791 - val_loss: 0.9987 - val_accuracy: 0.4794\n",
      "Epoch 766/1000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4440 - accuracy: 0.7894 - val_loss: 1.0044 - val_accuracy: 0.4925\n",
      "Epoch 767/1000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4515 - accuracy: 0.7897 - val_loss: 1.0103 - val_accuracy: 0.5000\n",
      "Epoch 768/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4594 - accuracy: 0.7809 - val_loss: 0.9986 - val_accuracy: 0.5044\n",
      "Epoch 769/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4450 - accuracy: 0.7827 - val_loss: 0.9965 - val_accuracy: 0.4975\n",
      "Epoch 770/1000\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.4513 - accuracy: 0.7827 - val_loss: 1.0078 - val_accuracy: 0.4950\n",
      "Epoch 771/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4396 - accuracy: 0.7969 - val_loss: 0.9990 - val_accuracy: 0.4994\n",
      "Epoch 772/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4569 - accuracy: 0.7827 - val_loss: 1.0004 - val_accuracy: 0.4938\n",
      "Epoch 773/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4519 - accuracy: 0.7850 - val_loss: 1.0006 - val_accuracy: 0.5075\n",
      "Epoch 774/1000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4437 - accuracy: 0.7967 - val_loss: 1.0116 - val_accuracy: 0.4913\n",
      "Epoch 775/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4446 - accuracy: 0.7962 - val_loss: 1.0192 - val_accuracy: 0.5019\n",
      "Epoch 776/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4541 - accuracy: 0.7841 - val_loss: 0.9971 - val_accuracy: 0.5025\n",
      "Epoch 777/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4557 - accuracy: 0.7848 - val_loss: 0.9986 - val_accuracy: 0.5075\n",
      "Epoch 778/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4535 - accuracy: 0.7805 - val_loss: 0.9905 - val_accuracy: 0.5081\n",
      "Epoch 779/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4426 - accuracy: 0.7905 - val_loss: 1.0033 - val_accuracy: 0.4994\n",
      "Epoch 780/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4468 - accuracy: 0.7833 - val_loss: 0.9977 - val_accuracy: 0.5038\n",
      "Epoch 781/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4461 - accuracy: 0.7908 - val_loss: 0.9925 - val_accuracy: 0.5069\n",
      "Epoch 782/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4515 - accuracy: 0.7789 - val_loss: 0.9918 - val_accuracy: 0.5038\n",
      "Epoch 783/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4434 - accuracy: 0.7911 - val_loss: 1.0028 - val_accuracy: 0.5031\n",
      "Epoch 784/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4440 - accuracy: 0.7911 - val_loss: 0.9984 - val_accuracy: 0.5000\n",
      "Epoch 785/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4470 - accuracy: 0.7930 - val_loss: 1.0007 - val_accuracy: 0.5075\n",
      "Epoch 786/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4399 - accuracy: 0.7937 - val_loss: 1.0204 - val_accuracy: 0.5044\n",
      "Epoch 787/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4456 - accuracy: 0.7880 - val_loss: 1.0115 - val_accuracy: 0.5038\n",
      "Epoch 788/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4536 - accuracy: 0.7869 - val_loss: 1.0019 - val_accuracy: 0.5050\n",
      "Epoch 789/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4590 - accuracy: 0.7825 - val_loss: 0.9972 - val_accuracy: 0.5081\n",
      "Epoch 790/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4471 - accuracy: 0.7798 - val_loss: 0.9941 - val_accuracy: 0.5063\n",
      "Epoch 791/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4506 - accuracy: 0.7828 - val_loss: 1.0034 - val_accuracy: 0.5038\n",
      "Epoch 792/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4510 - accuracy: 0.7873 - val_loss: 1.0075 - val_accuracy: 0.5025\n",
      "Epoch 793/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4537 - accuracy: 0.7855 - val_loss: 0.9990 - val_accuracy: 0.5044\n",
      "Epoch 794/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4553 - accuracy: 0.7848 - val_loss: 0.9963 - val_accuracy: 0.4931\n",
      "Epoch 795/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4474 - accuracy: 0.7856 - val_loss: 1.0126 - val_accuracy: 0.4950\n",
      "Epoch 796/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4429 - accuracy: 0.7919 - val_loss: 1.0047 - val_accuracy: 0.5094\n",
      "Epoch 797/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4541 - accuracy: 0.7817 - val_loss: 1.0034 - val_accuracy: 0.4994\n",
      "Epoch 798/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4570 - accuracy: 0.7811 - val_loss: 1.0068 - val_accuracy: 0.4969\n",
      "Epoch 799/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4492 - accuracy: 0.7848 - val_loss: 1.0051 - val_accuracy: 0.4969\n",
      "Epoch 800/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4468 - accuracy: 0.7853 - val_loss: 1.0122 - val_accuracy: 0.4981\n",
      "Epoch 801/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4437 - accuracy: 0.7923 - val_loss: 1.0060 - val_accuracy: 0.4963\n",
      "Epoch 802/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4520 - accuracy: 0.7864 - val_loss: 1.0024 - val_accuracy: 0.5075\n",
      "Epoch 803/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4472 - accuracy: 0.7886 - val_loss: 0.9877 - val_accuracy: 0.5144\n",
      "Epoch 804/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4481 - accuracy: 0.7869 - val_loss: 0.9974 - val_accuracy: 0.5050\n",
      "Epoch 805/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4457 - accuracy: 0.7870 - val_loss: 1.0089 - val_accuracy: 0.5025\n",
      "Epoch 806/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4463 - accuracy: 0.7852 - val_loss: 1.0019 - val_accuracy: 0.5069\n",
      "Epoch 807/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4461 - accuracy: 0.7861 - val_loss: 0.9969 - val_accuracy: 0.5144\n",
      "Epoch 808/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4471 - accuracy: 0.7897 - val_loss: 1.0022 - val_accuracy: 0.5044\n",
      "Epoch 809/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4491 - accuracy: 0.7858 - val_loss: 0.9999 - val_accuracy: 0.5138\n",
      "Epoch 810/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4457 - accuracy: 0.7870 - val_loss: 1.0046 - val_accuracy: 0.5006\n",
      "Epoch 811/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4408 - accuracy: 0.7948 - val_loss: 1.0158 - val_accuracy: 0.5000\n",
      "Epoch 812/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4325 - accuracy: 0.8014 - val_loss: 1.0091 - val_accuracy: 0.5075\n",
      "Epoch 813/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4511 - accuracy: 0.7858 - val_loss: 1.0079 - val_accuracy: 0.5069\n",
      "Epoch 814/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4475 - accuracy: 0.7916 - val_loss: 1.0049 - val_accuracy: 0.5056\n",
      "Epoch 815/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4413 - accuracy: 0.7903 - val_loss: 0.9968 - val_accuracy: 0.5063\n",
      "Epoch 816/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4496 - accuracy: 0.7914 - val_loss: 1.0109 - val_accuracy: 0.5063\n",
      "Epoch 817/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4389 - accuracy: 0.7969 - val_loss: 1.0054 - val_accuracy: 0.5106\n",
      "Epoch 818/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4486 - accuracy: 0.7894 - val_loss: 1.0137 - val_accuracy: 0.5025\n",
      "Epoch 819/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4415 - accuracy: 0.7983 - val_loss: 1.0215 - val_accuracy: 0.5000\n",
      "Epoch 820/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4477 - accuracy: 0.7844 - val_loss: 1.0087 - val_accuracy: 0.5125\n",
      "Epoch 821/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4571 - accuracy: 0.7770 - val_loss: 1.0109 - val_accuracy: 0.4994\n",
      "Epoch 822/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4423 - accuracy: 0.7933 - val_loss: 1.0060 - val_accuracy: 0.5088\n",
      "Epoch 823/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4406 - accuracy: 0.7917 - val_loss: 1.0158 - val_accuracy: 0.5031\n",
      "Epoch 824/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4498 - accuracy: 0.7916 - val_loss: 1.0160 - val_accuracy: 0.5025\n",
      "Epoch 825/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4428 - accuracy: 0.7948 - val_loss: 1.0122 - val_accuracy: 0.5044\n",
      "Epoch 826/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4325 - accuracy: 0.7970 - val_loss: 1.0249 - val_accuracy: 0.5056\n",
      "Epoch 827/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4464 - accuracy: 0.7848 - val_loss: 1.0206 - val_accuracy: 0.5069\n",
      "Epoch 828/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4431 - accuracy: 0.7933 - val_loss: 1.0108 - val_accuracy: 0.5119\n",
      "Epoch 829/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4359 - accuracy: 0.8025 - val_loss: 1.0135 - val_accuracy: 0.5006\n",
      "Epoch 830/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4416 - accuracy: 0.7934 - val_loss: 1.0199 - val_accuracy: 0.5031\n",
      "Epoch 831/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4387 - accuracy: 0.7953 - val_loss: 1.0190 - val_accuracy: 0.4963\n",
      "Epoch 832/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4527 - accuracy: 0.7836 - val_loss: 1.0155 - val_accuracy: 0.4988\n",
      "Epoch 833/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4433 - accuracy: 0.7930 - val_loss: 1.0196 - val_accuracy: 0.4988\n",
      "Epoch 834/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4488 - accuracy: 0.7862 - val_loss: 1.0143 - val_accuracy: 0.4956\n",
      "Epoch 835/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4425 - accuracy: 0.7920 - val_loss: 1.0073 - val_accuracy: 0.4975\n",
      "Epoch 836/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4447 - accuracy: 0.7855 - val_loss: 1.0101 - val_accuracy: 0.5038\n",
      "Epoch 837/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4390 - accuracy: 0.7934 - val_loss: 1.0124 - val_accuracy: 0.5025\n",
      "Epoch 838/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4571 - accuracy: 0.7844 - val_loss: 1.0215 - val_accuracy: 0.4956\n",
      "Epoch 839/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4501 - accuracy: 0.7853 - val_loss: 1.0104 - val_accuracy: 0.5013\n",
      "Epoch 840/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4364 - accuracy: 0.7911 - val_loss: 1.0073 - val_accuracy: 0.5025\n",
      "Epoch 841/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4429 - accuracy: 0.7911 - val_loss: 1.0153 - val_accuracy: 0.4969\n",
      "Epoch 842/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4461 - accuracy: 0.7953 - val_loss: 1.0002 - val_accuracy: 0.5006\n",
      "Epoch 843/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4486 - accuracy: 0.7855 - val_loss: 1.0084 - val_accuracy: 0.5038\n",
      "Epoch 844/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4495 - accuracy: 0.7906 - val_loss: 1.0002 - val_accuracy: 0.5006\n",
      "Epoch 845/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4357 - accuracy: 0.7903 - val_loss: 1.0004 - val_accuracy: 0.5050\n",
      "Epoch 846/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4543 - accuracy: 0.7839 - val_loss: 1.0065 - val_accuracy: 0.5006\n",
      "Epoch 847/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4455 - accuracy: 0.7864 - val_loss: 1.0129 - val_accuracy: 0.4956\n",
      "Epoch 848/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4530 - accuracy: 0.7861 - val_loss: 1.0036 - val_accuracy: 0.5044\n",
      "Epoch 849/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4394 - accuracy: 0.7931 - val_loss: 1.0030 - val_accuracy: 0.5025\n",
      "Epoch 850/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4449 - accuracy: 0.7909 - val_loss: 1.0109 - val_accuracy: 0.5063\n",
      "Epoch 851/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4430 - accuracy: 0.7912 - val_loss: 1.0048 - val_accuracy: 0.5144\n",
      "Epoch 852/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4435 - accuracy: 0.7937 - val_loss: 1.0016 - val_accuracy: 0.5050\n",
      "Epoch 853/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4455 - accuracy: 0.7931 - val_loss: 1.0144 - val_accuracy: 0.5100\n",
      "Epoch 854/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4552 - accuracy: 0.7825 - val_loss: 1.0121 - val_accuracy: 0.5075\n",
      "Epoch 855/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4305 - accuracy: 0.7956 - val_loss: 0.9961 - val_accuracy: 0.5031\n",
      "Epoch 856/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4406 - accuracy: 0.7911 - val_loss: 0.9880 - val_accuracy: 0.5050\n",
      "Epoch 857/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4450 - accuracy: 0.7941 - val_loss: 1.0065 - val_accuracy: 0.4938\n",
      "Epoch 858/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4586 - accuracy: 0.7803 - val_loss: 1.0081 - val_accuracy: 0.5050\n",
      "Epoch 859/1000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4481 - accuracy: 0.7856 - val_loss: 0.9899 - val_accuracy: 0.5031\n",
      "Epoch 860/1000\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.4442 - accuracy: 0.7936 - val_loss: 1.0072 - val_accuracy: 0.5006\n",
      "Epoch 861/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4311 - accuracy: 0.7964 - val_loss: 0.9993 - val_accuracy: 0.5000\n",
      "Epoch 862/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4368 - accuracy: 0.7941 - val_loss: 1.0049 - val_accuracy: 0.5025\n",
      "Epoch 863/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4476 - accuracy: 0.7877 - val_loss: 1.0036 - val_accuracy: 0.5050\n",
      "Epoch 864/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4469 - accuracy: 0.7822 - val_loss: 1.0036 - val_accuracy: 0.4981\n",
      "Epoch 865/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4485 - accuracy: 0.7869 - val_loss: 0.9989 - val_accuracy: 0.5106\n",
      "Epoch 866/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4482 - accuracy: 0.7825 - val_loss: 1.0125 - val_accuracy: 0.4950\n",
      "Epoch 867/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4467 - accuracy: 0.7905 - val_loss: 1.0088 - val_accuracy: 0.5081\n",
      "Epoch 868/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4541 - accuracy: 0.7853 - val_loss: 1.0081 - val_accuracy: 0.5063\n",
      "Epoch 869/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4505 - accuracy: 0.7825 - val_loss: 1.0202 - val_accuracy: 0.4981\n",
      "Epoch 870/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4453 - accuracy: 0.7855 - val_loss: 1.0070 - val_accuracy: 0.5056\n",
      "Epoch 871/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4550 - accuracy: 0.7800 - val_loss: 0.9997 - val_accuracy: 0.5081\n",
      "Epoch 872/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4481 - accuracy: 0.7886 - val_loss: 1.0150 - val_accuracy: 0.5013\n",
      "Epoch 873/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4398 - accuracy: 0.7967 - val_loss: 1.0021 - val_accuracy: 0.5069\n",
      "Epoch 874/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4488 - accuracy: 0.7834 - val_loss: 1.0009 - val_accuracy: 0.5075\n",
      "Epoch 875/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4529 - accuracy: 0.7848 - val_loss: 1.0048 - val_accuracy: 0.5000\n",
      "Epoch 876/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4372 - accuracy: 0.7941 - val_loss: 1.0113 - val_accuracy: 0.5075\n",
      "Epoch 877/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4352 - accuracy: 0.7973 - val_loss: 1.0130 - val_accuracy: 0.4981\n",
      "Epoch 878/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4389 - accuracy: 0.7955 - val_loss: 1.0160 - val_accuracy: 0.4931\n",
      "Epoch 879/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4371 - accuracy: 0.7933 - val_loss: 1.0144 - val_accuracy: 0.4925\n",
      "Epoch 880/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4365 - accuracy: 0.7947 - val_loss: 1.0224 - val_accuracy: 0.5000\n",
      "Epoch 881/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4444 - accuracy: 0.7875 - val_loss: 1.0313 - val_accuracy: 0.4994\n",
      "Epoch 882/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4417 - accuracy: 0.7908 - val_loss: 1.0202 - val_accuracy: 0.4963\n",
      "Epoch 883/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4465 - accuracy: 0.7884 - val_loss: 1.0143 - val_accuracy: 0.4981\n",
      "Epoch 884/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4452 - accuracy: 0.7886 - val_loss: 1.0223 - val_accuracy: 0.5025\n",
      "Epoch 885/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4468 - accuracy: 0.7911 - val_loss: 1.0098 - val_accuracy: 0.5063\n",
      "Epoch 886/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4448 - accuracy: 0.7886 - val_loss: 1.0145 - val_accuracy: 0.5031\n",
      "Epoch 887/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4535 - accuracy: 0.7842 - val_loss: 1.0071 - val_accuracy: 0.5031\n",
      "Epoch 888/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4363 - accuracy: 0.7942 - val_loss: 1.0235 - val_accuracy: 0.4988\n",
      "Epoch 889/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4391 - accuracy: 0.7889 - val_loss: 1.0242 - val_accuracy: 0.4981\n",
      "Epoch 890/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4440 - accuracy: 0.7897 - val_loss: 1.0170 - val_accuracy: 0.4906\n",
      "Epoch 891/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4559 - accuracy: 0.7811 - val_loss: 1.0200 - val_accuracy: 0.4969\n",
      "Epoch 892/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4335 - accuracy: 0.7956 - val_loss: 1.0109 - val_accuracy: 0.4963\n",
      "Epoch 893/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4429 - accuracy: 0.7859 - val_loss: 1.0253 - val_accuracy: 0.4994\n",
      "Epoch 894/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4515 - accuracy: 0.7898 - val_loss: 1.0191 - val_accuracy: 0.5038\n",
      "Epoch 895/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4431 - accuracy: 0.7897 - val_loss: 1.0233 - val_accuracy: 0.4944\n",
      "Epoch 896/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4324 - accuracy: 0.7981 - val_loss: 1.0133 - val_accuracy: 0.5075\n",
      "Epoch 897/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4401 - accuracy: 0.7970 - val_loss: 1.0215 - val_accuracy: 0.5063\n",
      "Epoch 898/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4340 - accuracy: 0.7925 - val_loss: 1.0232 - val_accuracy: 0.5081\n",
      "Epoch 899/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4515 - accuracy: 0.7875 - val_loss: 1.0181 - val_accuracy: 0.5019\n",
      "Epoch 900/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4475 - accuracy: 0.7852 - val_loss: 1.0265 - val_accuracy: 0.5006\n",
      "Epoch 901/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4314 - accuracy: 0.7970 - val_loss: 1.0117 - val_accuracy: 0.5056\n",
      "Epoch 902/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4354 - accuracy: 0.7947 - val_loss: 1.0238 - val_accuracy: 0.4944\n",
      "Epoch 903/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4501 - accuracy: 0.7867 - val_loss: 1.0254 - val_accuracy: 0.4888\n",
      "Epoch 904/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4365 - accuracy: 0.7958 - val_loss: 1.0187 - val_accuracy: 0.4994\n",
      "Epoch 905/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4345 - accuracy: 0.7917 - val_loss: 1.0107 - val_accuracy: 0.4963\n",
      "Epoch 906/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4359 - accuracy: 0.7973 - val_loss: 1.0201 - val_accuracy: 0.5006\n",
      "Epoch 907/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4482 - accuracy: 0.7891 - val_loss: 1.0145 - val_accuracy: 0.5019\n",
      "Epoch 908/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4429 - accuracy: 0.7909 - val_loss: 1.0278 - val_accuracy: 0.4994\n",
      "Epoch 909/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4403 - accuracy: 0.7883 - val_loss: 1.0389 - val_accuracy: 0.5050\n",
      "Epoch 910/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4487 - accuracy: 0.7944 - val_loss: 1.0317 - val_accuracy: 0.5056\n",
      "Epoch 911/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4460 - accuracy: 0.7902 - val_loss: 1.0332 - val_accuracy: 0.5031\n",
      "Epoch 912/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4343 - accuracy: 0.7939 - val_loss: 1.0263 - val_accuracy: 0.5031\n",
      "Epoch 913/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4485 - accuracy: 0.7916 - val_loss: 1.0311 - val_accuracy: 0.5019\n",
      "Epoch 914/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4564 - accuracy: 0.7833 - val_loss: 1.0079 - val_accuracy: 0.5019\n",
      "Epoch 915/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4465 - accuracy: 0.7922 - val_loss: 1.0184 - val_accuracy: 0.5006\n",
      "Epoch 916/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4362 - accuracy: 0.8019 - val_loss: 1.0165 - val_accuracy: 0.4988\n",
      "Epoch 917/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4502 - accuracy: 0.7855 - val_loss: 1.0115 - val_accuracy: 0.5069\n",
      "Epoch 918/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4417 - accuracy: 0.7908 - val_loss: 1.0189 - val_accuracy: 0.5000\n",
      "Epoch 919/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4492 - accuracy: 0.7945 - val_loss: 1.0210 - val_accuracy: 0.5006\n",
      "Epoch 920/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4454 - accuracy: 0.7897 - val_loss: 1.0217 - val_accuracy: 0.5031\n",
      "Epoch 921/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4330 - accuracy: 0.7947 - val_loss: 1.0145 - val_accuracy: 0.5056\n",
      "Epoch 922/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4359 - accuracy: 0.7969 - val_loss: 1.0291 - val_accuracy: 0.4988\n",
      "Epoch 923/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4295 - accuracy: 0.8005 - val_loss: 1.0162 - val_accuracy: 0.5063\n",
      "Epoch 924/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4444 - accuracy: 0.7939 - val_loss: 1.0235 - val_accuracy: 0.5025\n",
      "Epoch 925/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4356 - accuracy: 0.7991 - val_loss: 1.0082 - val_accuracy: 0.5100\n",
      "Epoch 926/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4462 - accuracy: 0.7859 - val_loss: 0.9951 - val_accuracy: 0.5144\n",
      "Epoch 927/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4353 - accuracy: 0.8023 - val_loss: 1.0097 - val_accuracy: 0.5100\n",
      "Epoch 928/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4417 - accuracy: 0.7875 - val_loss: 1.0028 - val_accuracy: 0.5050\n",
      "Epoch 929/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4324 - accuracy: 0.8012 - val_loss: 1.0184 - val_accuracy: 0.4994\n",
      "Epoch 930/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4482 - accuracy: 0.7887 - val_loss: 1.0169 - val_accuracy: 0.4975\n",
      "Epoch 931/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4418 - accuracy: 0.7900 - val_loss: 1.0165 - val_accuracy: 0.4981\n",
      "Epoch 932/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4414 - accuracy: 0.7858 - val_loss: 1.0101 - val_accuracy: 0.5150\n",
      "Epoch 933/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4419 - accuracy: 0.7942 - val_loss: 1.0135 - val_accuracy: 0.5038\n",
      "Epoch 934/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4480 - accuracy: 0.7830 - val_loss: 1.0208 - val_accuracy: 0.5131\n",
      "Epoch 935/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4416 - accuracy: 0.7922 - val_loss: 1.0165 - val_accuracy: 0.4944\n",
      "Epoch 936/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4319 - accuracy: 0.7989 - val_loss: 1.0188 - val_accuracy: 0.5000\n",
      "Epoch 937/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4351 - accuracy: 0.7977 - val_loss: 1.0193 - val_accuracy: 0.5006\n",
      "Epoch 938/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4361 - accuracy: 0.7884 - val_loss: 1.0147 - val_accuracy: 0.4994\n",
      "Epoch 939/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4384 - accuracy: 0.7945 - val_loss: 1.0093 - val_accuracy: 0.5000\n",
      "Epoch 940/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4460 - accuracy: 0.7916 - val_loss: 1.0208 - val_accuracy: 0.5056\n",
      "Epoch 941/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4448 - accuracy: 0.7927 - val_loss: 1.0312 - val_accuracy: 0.5006\n",
      "Epoch 942/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4489 - accuracy: 0.7833 - val_loss: 1.0130 - val_accuracy: 0.4994\n",
      "Epoch 943/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4461 - accuracy: 0.7852 - val_loss: 1.0200 - val_accuracy: 0.5000\n",
      "Epoch 944/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4436 - accuracy: 0.7869 - val_loss: 1.0125 - val_accuracy: 0.5056\n",
      "Epoch 945/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4423 - accuracy: 0.7953 - val_loss: 1.0193 - val_accuracy: 0.4994\n",
      "Epoch 946/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4354 - accuracy: 0.7970 - val_loss: 1.0201 - val_accuracy: 0.5031\n",
      "Epoch 947/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4418 - accuracy: 0.7906 - val_loss: 1.0186 - val_accuracy: 0.5050\n",
      "Epoch 948/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4297 - accuracy: 0.8022 - val_loss: 1.0386 - val_accuracy: 0.5031\n",
      "Epoch 949/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4448 - accuracy: 0.7917 - val_loss: 1.0127 - val_accuracy: 0.4994\n",
      "Epoch 950/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4320 - accuracy: 0.7925 - val_loss: 1.0359 - val_accuracy: 0.5025\n",
      "Epoch 951/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4442 - accuracy: 0.7920 - val_loss: 1.0210 - val_accuracy: 0.4981\n",
      "Epoch 952/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4256 - accuracy: 0.8053 - val_loss: 1.0243 - val_accuracy: 0.5063\n",
      "Epoch 953/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4370 - accuracy: 0.7881 - val_loss: 1.0325 - val_accuracy: 0.5050\n",
      "Epoch 954/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4388 - accuracy: 0.7955 - val_loss: 1.0288 - val_accuracy: 0.4975\n",
      "Epoch 955/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4353 - accuracy: 0.7986 - val_loss: 1.0226 - val_accuracy: 0.5081\n",
      "Epoch 956/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4353 - accuracy: 0.7970 - val_loss: 1.0338 - val_accuracy: 0.5094\n",
      "Epoch 957/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4461 - accuracy: 0.7877 - val_loss: 1.0262 - val_accuracy: 0.5031\n",
      "Epoch 958/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4587 - accuracy: 0.7905 - val_loss: 1.0230 - val_accuracy: 0.4969\n",
      "Epoch 959/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4445 - accuracy: 0.7905 - val_loss: 1.0222 - val_accuracy: 0.5013\n",
      "Epoch 960/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4516 - accuracy: 0.7880 - val_loss: 1.0256 - val_accuracy: 0.4906\n",
      "Epoch 961/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4391 - accuracy: 0.7947 - val_loss: 1.0254 - val_accuracy: 0.5050\n",
      "Epoch 962/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4389 - accuracy: 0.7927 - val_loss: 1.0208 - val_accuracy: 0.5006\n",
      "Epoch 963/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4314 - accuracy: 0.7952 - val_loss: 1.0185 - val_accuracy: 0.5025\n",
      "Epoch 964/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4367 - accuracy: 0.7917 - val_loss: 1.0233 - val_accuracy: 0.5000\n",
      "Epoch 965/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4447 - accuracy: 0.7912 - val_loss: 1.0190 - val_accuracy: 0.5038\n",
      "Epoch 966/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4443 - accuracy: 0.7873 - val_loss: 1.0165 - val_accuracy: 0.4988\n",
      "Epoch 967/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4272 - accuracy: 0.7977 - val_loss: 1.0161 - val_accuracy: 0.5013\n",
      "Epoch 968/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4426 - accuracy: 0.7870 - val_loss: 1.0175 - val_accuracy: 0.5106\n",
      "Epoch 969/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4481 - accuracy: 0.7908 - val_loss: 1.0203 - val_accuracy: 0.5038\n",
      "Epoch 970/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4378 - accuracy: 0.7984 - val_loss: 1.0304 - val_accuracy: 0.5000\n",
      "Epoch 971/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4404 - accuracy: 0.7916 - val_loss: 1.0149 - val_accuracy: 0.5025\n",
      "Epoch 972/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4355 - accuracy: 0.7981 - val_loss: 1.0278 - val_accuracy: 0.5019\n",
      "Epoch 973/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4464 - accuracy: 0.7884 - val_loss: 1.0232 - val_accuracy: 0.5038\n",
      "Epoch 974/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4416 - accuracy: 0.7925 - val_loss: 1.0167 - val_accuracy: 0.4988\n",
      "Epoch 975/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4321 - accuracy: 0.7991 - val_loss: 1.0294 - val_accuracy: 0.5038\n",
      "Epoch 976/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4422 - accuracy: 0.7952 - val_loss: 1.0206 - val_accuracy: 0.5088\n",
      "Epoch 977/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4445 - accuracy: 0.7828 - val_loss: 1.0323 - val_accuracy: 0.5056\n",
      "Epoch 978/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4374 - accuracy: 0.7917 - val_loss: 1.0178 - val_accuracy: 0.5144\n",
      "Epoch 979/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4465 - accuracy: 0.7889 - val_loss: 1.0178 - val_accuracy: 0.5106\n",
      "Epoch 980/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4401 - accuracy: 0.7950 - val_loss: 1.0209 - val_accuracy: 0.5075\n",
      "Epoch 981/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4442 - accuracy: 0.7872 - val_loss: 1.0108 - val_accuracy: 0.5088\n",
      "Epoch 982/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4451 - accuracy: 0.7892 - val_loss: 1.0073 - val_accuracy: 0.5113\n",
      "Epoch 983/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4334 - accuracy: 0.7980 - val_loss: 1.0112 - val_accuracy: 0.5081\n",
      "Epoch 984/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4309 - accuracy: 0.7983 - val_loss: 1.0149 - val_accuracy: 0.5038\n",
      "Epoch 985/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4409 - accuracy: 0.7948 - val_loss: 1.0117 - val_accuracy: 0.5100\n",
      "Epoch 986/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4349 - accuracy: 0.7905 - val_loss: 1.0170 - val_accuracy: 0.5156\n",
      "Epoch 987/1000\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.4382 - accuracy: 0.7962 - val_loss: 1.0103 - val_accuracy: 0.5013\n",
      "Epoch 988/1000\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.4477 - accuracy: 0.7883 - val_loss: 1.0199 - val_accuracy: 0.5031\n",
      "Epoch 989/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4377 - accuracy: 0.7922 - val_loss: 1.0228 - val_accuracy: 0.4988\n",
      "Epoch 990/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4481 - accuracy: 0.7847 - val_loss: 1.0216 - val_accuracy: 0.4944\n",
      "Epoch 991/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4420 - accuracy: 0.7895 - val_loss: 1.0113 - val_accuracy: 0.5044\n",
      "Epoch 992/1000\n",
      "200/200 [==============================] - 2s 9ms/step - loss: 0.4263 - accuracy: 0.8014 - val_loss: 1.0196 - val_accuracy: 0.4988\n",
      "Epoch 993/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4377 - accuracy: 0.7906 - val_loss: 1.0208 - val_accuracy: 0.5069\n",
      "Epoch 994/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4373 - accuracy: 0.7953 - val_loss: 1.0253 - val_accuracy: 0.5094\n",
      "Epoch 995/1000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4283 - accuracy: 0.8031 - val_loss: 1.0225 - val_accuracy: 0.4981\n",
      "Epoch 996/1000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4312 - accuracy: 0.8002 - val_loss: 1.0180 - val_accuracy: 0.5050\n",
      "Epoch 997/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4333 - accuracy: 0.7975 - val_loss: 1.0319 - val_accuracy: 0.4988\n",
      "Epoch 998/1000\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4428 - accuracy: 0.7905 - val_loss: 1.0232 - val_accuracy: 0.5100\n",
      "Epoch 999/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4349 - accuracy: 0.7970 - val_loss: 1.0198 - val_accuracy: 0.5031\n",
      "Epoch 1000/1000\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4424 - accuracy: 0.7942 - val_loss: 1.0166 - val_accuracy: 0.5019\n",
      "\n",
      "Model saved successfully to 'best_model.keras'\n",
      "63/63 [==============================] - 0s 2ms/step\n",
      "\n",
      "--- Neural Network Model Performance Metrics ---\n",
      "Accuracy: 0.5170\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.51      0.53      0.52       989\n",
      "         1.0       0.52      0.51      0.51      1011\n",
      "\n",
      "    accuracy                           0.52      2000\n",
      "   macro avg       0.52      0.52      0.52      2000\n",
      "weighted avg       0.52      0.52      0.52      2000\n",
      "\n",
      "ROC AUC Score: 0.5069\n",
      "\n",
      "--- Regression Metrics on Predicted Probabilities ---\n",
      "R-squared (R2): -0.3468\n",
      "Mean Absolute Error (MAE): 0.4962\n",
      "Mean Squared Error (MSE): 0.3367\n",
      "Root Mean Squared Error (RMSE): 0.5802\n",
      "Mean Absolute Percentage Error (MAPE): 2378318411.62%\n",
      "Mean Squared Log Error (MSLE): 0.1623\n",
      "\n",
      "Model performance metrics saved to 'nn_performance_metrics.xlsx'\n",
      "\n",
      "Confusion matrix plot saved to 'confusion_matrix.svg'\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perform_neural_network_classification(\"../mapped_dataset_Normalized_version.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912a1844-cf79-4cb9-ac68-192bc5e2babd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
