{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b93b032-aa3a-4428-94c1-cd89493c11c1",
   "metadata": {},
   "source": [
    "# the version Five of our classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c9e187d-63b0-41c2-b247-c692abf28b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, classification_report,\n",
    "    roc_auc_score, r2_score, mean_squared_error, mean_absolute_error,\n",
    "    mean_absolute_percentage_error, mean_squared_log_error\n",
    ")\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def perform_neural_network_classification(csv_file_path):\n",
    "    \"\"\"\n",
    "    Performs neural network binary classification on a dataset, calculates\n",
    "    various classification and regression metrics, generates a confusion\n",
    "    matrix heatmap, and saves all metrics to an Excel file.\n",
    "\n",
    "    This version includes an improved model architecture and training\n",
    "    process with regularization and callbacks to prevent overfitting and\n",
    "    optimize performance.\n",
    "\n",
    "    Args:\n",
    "        csv_file_path (str): The path to the CSV file. All columns except the last\n",
    "                             are treated as features (X), and the last column,\n",
    "                             which should contain 0s and 1s, is the target variable (y).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the dataset\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "        # Separate features (X) and target (y)\n",
    "        X = df.iloc[:, :-1]  # All columns except the last\n",
    "        y = df.iloc[:, -1]   # The last column (0 or 1)\n",
    "\n",
    "        # Split the data into training and testing sets (80/20 split)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Standardize the data to help the neural network converge faster\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # --- Define Callbacks for Training ---\n",
    "        # EarlyStopping: Stop training when validation loss stops improving for a certain number of epochs.\n",
    "        # This prevents overfitting and saves training time.\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=50,  # Number of epochs with no improvement after which training will be stopped\n",
    "            restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    "        )\n",
    "\n",
    "        # ReduceLROnPlateau: Reduce the learning rate when a metric has stopped improving.\n",
    "        # This can help the model find a better minimum in the loss function.\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2, # Factor by which the learning rate will be reduced\n",
    "            patience=20, # Number of epochs with no improvement after which learning rate will be reduced\n",
    "            min_lr=0.00001\n",
    "        )\n",
    "\n",
    "        # --- Improved Neural Network Model Setup ---\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Input layer and first hidden layer with L2 regularization to prevent overfitting\n",
    "        model.add(Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],),\n",
    "                        kernel_regularizer=l2(0.001))) # L2 regularization\n",
    "\n",
    "        # Dropout layer to prevent overfitting\n",
    "        model.add(Dropout(0.4))\n",
    "        \n",
    "        # Second hidden layer\n",
    "        model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "\n",
    "        model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "\n",
    "        model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "\n",
    "        model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "\n",
    "        model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "\n",
    "        model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "\n",
    "        model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "\n",
    "        model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "\n",
    "        model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "\n",
    "        # Output layer for binary classification with a single neuron and sigmoid activation\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        # Compile the model with the Adam optimizer and binary cross-entropy loss\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Print the model summary\n",
    "        print(\"Model Summary:\")\n",
    "        model.summary()\n",
    "        \n",
    "        # Train the model with the added callbacks\n",
    "        print(\"\\nTraining Neural Network model...\")\n",
    "        history = model.fit(\n",
    "            X_train_scaled, y_train,\n",
    "            epochs=2000,  # Set a high number of epochs, but EarlyStopping will handle stopping\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,  # Use 20% of the training data for validation\n",
    "            callbacks=[early_stopping, reduce_lr], # Pass the callbacks here\n",
    "            verbose=1  # Show training progress\n",
    "        )\n",
    "        \n",
    "        # --- Save the Trained Model ---\n",
    "        # Save the entire model (architecture, weights, and optimizer state)\n",
    "        model_path = 'best_model.keras'\n",
    "        model.save(model_path)\n",
    "        print(f\"\\nModel saved successfully to '{model_path}'\")\n",
    "\n",
    "        # --- Make Predictions ---\n",
    "        # The model predicts a probability. We round it to get a binary class (0 or 1).\n",
    "        y_pred_proba = model.predict(X_test_scaled).flatten()\n",
    "        y_pred = (y_pred_proba > 0.5).astype(\"int32\")\n",
    "\n",
    "        # --- 1. Calculate Classification Metrics ---\n",
    "        print(\"\\n--- Neural Network Model Performance Metrics ---\")\n",
    "\n",
    "        # Accuracy Score\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # Classification Report (Precision, Recall, F1-Score)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        # ROC AUC Score\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "        \n",
    "        # --- 2. Calculate Regression Metrics on Probabilities ---\n",
    "        print(\"\\n--- Regression Metrics on Predicted Probabilities ---\")\n",
    "\n",
    "        # R-squared (Coefficient of Determination)\n",
    "        r2 = r2_score(y_test, y_pred_proba)\n",
    "        print(f\"R-squared (R2): {r2:.4f}\")\n",
    "\n",
    "        # Mean Absolute Error (MAE)\n",
    "        mae = mean_absolute_error(y_test, y_pred_proba)\n",
    "        print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "\n",
    "        # Mean Squared Error (MSE)\n",
    "        mse = mean_squared_error(y_test, y_pred_proba)\n",
    "        print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "\n",
    "        # Root Mean Squared Error (RMSE)\n",
    "        rmse = np.sqrt(mse)\n",
    "        print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "\n",
    "        # Mean Absolute Percentage Error (MAPE)\n",
    "        mape = np.mean(np.abs((y_test - y_pred_proba) / (y_test + 1e-8))) * 100\n",
    "        print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "\n",
    "        # Mean Squared Log Error (MSLE) - check for negative values\n",
    "        msle = mean_squared_log_error(y_test + 1e-8, y_pred_proba + 1e-8)\n",
    "        print(f\"Mean Squared Log Error (MSLE): {msle:.4f}\")\n",
    "\n",
    "        # --- 3. Save Metrics to Excel ---\n",
    "        metrics_data = {\n",
    "            'Metric': ['Accuracy', 'ROC AUC', 'R2 Score', 'MAE', 'MSE', 'RMSE', 'MAPE', 'MSLE'],\n",
    "            'Value': [accuracy, roc_auc, r2, mae, mse, rmse, mape, msle]\n",
    "        }\n",
    "        \n",
    "        metrics_df = pd.DataFrame(metrics_data)\n",
    "        excel_path = 'nn_performance_metrics.xlsx'\n",
    "        metrics_df.to_excel(excel_path, index=False)\n",
    "        print(f\"\\nModel performance metrics saved to '{excel_path}'\")\n",
    "\n",
    "        # --- 4. Generate Confusion Matrix Plot ---\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        plt.figure(figsize=(8, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                    xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "                    yticklabels=['Actual 0', 'Actual 1'])\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        \n",
    "        plot_path = 'confusion_matrix.svg'\n",
    "        plt.savefig(plot_path, format='svg')\n",
    "        print(f\"\\nConfusion matrix plot saved to '{plot_path}'\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        # Plot training & validation loss values\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{csv_file_path}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example Usage: Uncomment the line below and provide the path to your CSV file\n",
    "# if __name__ == \"__main__\":\n",
    "#     perform_neural_network_classification(\"path/to/your/data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d524fe34-6f70-4861-b978-32f5a280fbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Model Summary:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               1792      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 43393 (169.50 KB)\n",
      "Trainable params: 43393 (169.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "Training Neural Network model...\n",
      "Epoch 1/2000\n",
      "WARNING:tensorflow:From C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\acking\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "200/200 [==============================] - 8s 9ms/step - loss: 0.9101 - accuracy: 0.4980 - val_loss: 0.7229 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 2/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7012 - accuracy: 0.4975 - val_loss: 0.6937 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 3/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6933 - accuracy: 0.5013 - val_loss: 0.6931 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 4/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 5/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.4888 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 6/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.4969 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 7/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.4928 - val_loss: 0.6932 - val_accuracy: 0.4762 - lr: 0.0010\n",
      "Epoch 8/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.4934 - val_loss: 0.6931 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 9/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.4988 - val_loss: 0.6931 - val_accuracy: 0.4762 - lr: 0.0010\n",
      "Epoch 10/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.5009 - val_loss: 0.6931 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 11/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6931 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 12/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.4972 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 13/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 14/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6931 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 15/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.4956 - val_loss: 0.6931 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 16/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.4909 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 17/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.4944 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 18/2000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.6932 - accuracy: 0.4959 - val_loss: 0.6929 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 19/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.4906 - val_loss: 0.6929 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 20/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 21/2000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.6932 - accuracy: 0.4956 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 22/2000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6931 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 23/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6931 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 24/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.4988 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 25/2000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 26/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 27/2000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.6932 - accuracy: 0.4928 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 28/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.4950 - val_loss: 0.6929 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 29/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 30/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 31/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.4913 - val_loss: 0.6931 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 32/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.4953 - val_loss: 0.6931 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 33/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 34/2000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.6932 - accuracy: 0.4944 - val_loss: 0.6931 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 35/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 36/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.4950 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 37/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6931 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 38/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.4944 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 39/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 2.0000e-04\n",
      "Epoch 40/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 2.0000e-04\n",
      "Epoch 41/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6931 - accuracy: 0.5013 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 2.0000e-04\n",
      "Epoch 42/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 2.0000e-04\n",
      "Epoch 43/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 2.0000e-04\n",
      "Epoch 44/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 2.0000e-04\n",
      "Epoch 45/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 2.0000e-04\n",
      "Epoch 46/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6931 - accuracy: 0.5013 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 2.0000e-04\n",
      "Epoch 47/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 2.0000e-04\n",
      "Epoch 48/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 2.0000e-04\n",
      "Epoch 49/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 2.0000e-04\n",
      "Epoch 50/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 2.0000e-04\n",
      "Epoch 51/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6931 - accuracy: 0.5013 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 2.0000e-04\n",
      "Epoch 52/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 2.0000e-04\n",
      "Epoch 53/2000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 2.0000e-04\n",
      "Epoch 54/2000\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.6931 - accuracy: 0.5013 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 2.0000e-04\n",
      "Epoch 55/2000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6931 - accuracy: 0.5013 - val_loss: 0.6930 - val_accuracy: 0.5238 - lr: 2.0000e-04\n",
      "Epoch 56/2000\n",
      " 23/200 [==>...........................] - ETA: 0s - loss: 0.6931 - accuracy: 0.5204"
     ]
    }
   ],
   "source": [
    "perform_neural_network_classification(\"../../mapped_dataset_Normalized_version.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912a1844-cf79-4cb9-ac68-192bc5e2babd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
